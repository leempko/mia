["Medical Image Analysis Koen Van Leemput September 16, 2024 ","c  This work is licensed under CC BY 4.0 ","Contents 1 Smoothing and Interpolation 1 1.1 Linear regression ........................... 1 1.1.1 Regularization ........................ 2 1.2 Smoothing and interpolation of 1D signals ............. 3 1.2.1 Smoothing .......................... 5 1.2.2 Interpolation ......................... 7 1.3 Smoothing and interpolation in higher dimensions ........ 7 1.3.1 Exploiting separability .................... 10 1.3.2 Smoothing in 2D ....................... 11 1.3.3 Interpolation in 2D ...................... 11 2 Image Registration 15 2.1 Coordinate systems .......................... 15 2.2 Transformation models ........................ 18 2.2.1 Linear transformations .................... 18 2.2.2 Nonlinear transformations .................. 21 2.3 Landmark-based registration .................... 21 2.4 Intensity-based registration ..................... 23 2.4.1 Sum of squared differences .................. 23 2.4.2 Mutual Information ..................... 26 3 Model-based Segmentation 31 3.1 Generative models .......................... 31 3.2 Gaussian mixture model ....................... 32 3.3 Markov random field priors ..................... 36 3.3.1 Markov random field model ................. 36 3.3.2 Inference using the mean-field approximation ....... 38 3.4 Parameter optimization using the EM algorithm ......... 40 3.5 Modeling MR bias fields ....................... 45 4 Neural Networks 51 4.1 Logistic regression .......................... 51 4.2 Training with stochastic gradient descent ............. 53 4.3 Feed-forward network functions ................... 55 i ","ii CONTENTS 5 Atlases 61 5.1 Reference templates ......................... 61 5.1.1 Intensity averaging ...................... 62 5.1.2 Group-wise registration ................... 62 5.2 Atlases for segmentation ....................... 64 5.2.1 Probabilistic atlases ..................... 64 5.2.2 Label propagation ...................... 67 6 Validation 69 6.1 Validation against a known ground truth .............. 69 6.1.1 Confusion matrix, sensitivity, and specificity ........ 69 6.1.2 ROC curve .......................... 71 6.1.3 Dice score ........................... 74 6.2 Estimating the ground truth ..................... 74 Bibliography 81 ","Chapter 1 Smoothing and Interpolation A fundamental prerequisite for solving many medical image analysis tasks is the ability to smooth and interpolate signals in two or three dimensions. In this chapter the basic principles of these techniques are reviewed. 1.1 Linear regression Let x =(x 1 ,...,x D ) T denote the spatial position in a D-dimensional space. In medical imaging, D is typically 2 or 3. Given N measurements {t n } N n=1 at locations {x n } N n=1 , a frequent task is to predict the value t at a new location x. A simple model, known as linear regression, uses the function value y(x, w)= w 0 + w 1 x 1 + ... + w D x D as its prediction, where w 0 ,...,w D are tunable weights that need to be esti- mated from the available measurements. A more general form uses nonlinear functions of the input locations instead: y(x, w)= w 0 + M-1 X m=1 w m φ m (x), which greatly increases the flexibility of the model. Here the functions φ m (x) are known as basis functions, and it is often convenient to define an additional “dummy” basis function φ 0 (x) = 1, so that the model can be written as y(x, w)= M-1 X m=0 w m φ m (x), (1.1) where w =(w 0 ,...,w M-1 ) T are M tunable parameters. 1 ","2 CHAPTER 1. SMOOTHING AND INTERPOLATION In order to find suitable values of the parameters of the model, the following energy can be minimized with respect to w: E(w)= N X n=1  t n - M-1 X m=0 w m φ m (x n ) ! 2 , which simply sums of the squared distances between the measurements t n and the model’s predictions y(x n , w). Taking the partial derivative with respect to parameter w m yields ∂E(w) ∂w m = -2 N X n=1  t n - M-1 X m=0 w m φ m (x n ) ! φ m (x n ), so that the gradient is given by ∇E(w)=     ∂E(w) ∂w0 . . . ∂E(w) ∂w M-1     = -2Φ T (t - Φw) , where t =(t 1 ,...,t N ) T is a vector stacking all measurements, and Φ =      φ 0 (x 1 ) φ 1 (x 1 ) ... φ M-1 (x 1 ) φ 0 (x 2 ) φ 1 (x 2 ) ... φ M-1 (x 2 ) . . . . . . . . . . . . φ 0 (x N ) φ 1 (x N ) ... φ M-1 (x N )      (1.2) is a N × M matrix containing the value of all the basis functions in each of the measurement locations. Settings this gradient to zero gives Φ T (t - Φw)= 0 with solution w =  Φ T Φ  -1 Φ T t. (1.3) With these parameters, a prediction at a new location x is obtained by evalu- ating (1.1). 1.1.1 Regularization In some cases prior knowledge is available about the parameters w (for instance, that their values are typically small), or about the expected behavior of y(x, w) (for instance that it is a smooth function). This can be taken into account by adding a regularization term to the energy, optimizing E(w)+ λkΩwk 2 (1.4) ","1.2. SMOOTHING AND INTERPOLATION OF 1D SIGNALS 3 instead. Here λ is a parameter controlling the strength of the regularization, and Ω is a matrix chosen so that it penalizes undesirable values of w. An example of Ω could simply be the identify matrix, so that the regularization energy becomes kwk 2 = w 2 0 + ... + w 2 M-1 , which penalizes large values of w. Other examples include Ω = Φ, so that large values of the predictions ˆ t = Φw at the measurement locations are penalized; or even Ω = ΓΦ, with Γ detecting rapid spatial fluctuations in ˆ t (e.g., first- or second-order finite differences). Optimizing (1.4) with respect to w, by setting its gradient to zero and solving for w as before, yields w =  Φ T Φ + λΩ T Ω  -1 Φ T t, (1.5) which is a straightforward extension of (1.3) 1.2 Smoothing and interpolation of 1D signals Before addressing applications in two- or three-dimensional imaging problems, it is instructive to first study the one-dimensional (1D) setting. In this scenario, we are seeking functions of the form y(x, w)= M-1 X m=0 w m φ m (x), (1.6) where x is a scalar (a location in 1D space). Furthermore, the N measurement locations lie on a regular grid with unit interval: x n = n - 1, ∀n =1,...,N, i.e., the signals t are 1D “images”. In signal processing applications, the basis functions associated with the “type II” discrete cosine transform (DCT) often have useful analytical and numerical properties. In particular, they often allow to compress signals ef- ficiently by discarding the highest frequencies; Φw and Φ T t can be computed very quickly using the fast Fourier transform (FTT); and Φ T Φ = N 2 I. They are defined as follows: φ m (x)=  1 √ 2  [m=0] cos  πm(x + 1 2 )/N  , (1.7) for frequencies m =0,...,M - 1, where M ≤ N . An example is shown in Fig. 1.1a for M = 5. ","4 CHAPTER 1. SMOOTHING AND INTERPOLATION (a) DCT (b) B-spline Figure 1.1: Two sets of often-used basis functions (M = 5). Another set of useful basis functions are derived from B-splines, which are symmetrical functions constructed from the repeated convolution of a rectan- gular pulse β 0 : β 0 (x) =    1, - 1 2 <x< 1 2 1 2 , |x| = 1 2 0, otherwise, β p (x) = ( β 0 * β 0 *···* β 0 ) | {z } (p+1) times (x). Here p is called the order of the B-spline, and * denotes a convolution: (f * g)(x)= Z ∞ τ =-∞ f (τ )g(x - τ )dτ. Often-used B-spline orders (e.g, for interpolation) are p = 1: β 1 (x)=  1 -|x|, |x| < 1 0, otherwise, and p = 3 (“cubic” B-spline): β 3 (x)=      2 3 -|x| 2 + |x| 3 2 , |x| < 1 (2-|x|) 3 6 , 1 ≤|x| < 2 0, otherwise. B-splines of order zero, one and three are illustrated in Fig. 1.2. B-splines are typically used to construct basis functions by scaling them with some factor h, and shifting them to be h unites apart: φ m (x)= β p  x - mh h  . (1.8) ","1.2. SMOOTHING AND INTERPOLATION OF 1D SIGNALS 5 Figure 1.2: B-splines β 0 (x), β 1 (x) and β 3 (x). This is illustrated in Fig. 1.1b for the case h = 25. The usefulness of these type of basis functions stems from their limited support (they are only non-zero within a small range), which can be used to dramatically reduce the number of terms that need to be summed over in practical implementations (e.g., in (1.6)). Furthermore, in interpolation applications B-splines enjoy a number of theo- retical and numerical advantages (e.g., the curvature of y(x, w) is minimized, and the required matrix inversions can be performed with very fast filter-based methods) [1]. 1.2.1 Smoothing In some applications, the measurements t n ,n =1,...,N are only noisy ob- servations, and the aim is to recover the “denoised” underlying signal ˆ t n = y(x n , w) at the locations x n . Collecting these denoised estimates into a vector ˆ t =( ˆ t 1 ,..., ˆ t N ) T , we obtain (cf. (1.6)): ˆ t = Φw. (1.9) Plugging in the solution (1.5) then yields ˆ t = St, where S = Φ  Φ T Φ + λΩ T Ω  -1 Φ T is a N ×N “smoothing” matrix that transforms a noisy signal t into a “denoised” signal ˆ t by making linear combinations of the elements in t. The n-th row of S contains the weights assigned to the various elements of t in the computation of ˆ t n , as illustrated in Fig. 1.3 (bottom row). The amount of smoothing that is applied can be controlled by both the number of basis functions M and the regularization parameter λ: choosing higher M or lower λ will produce less smoothing. This effect is illustrated in Fig. 1.3. In the limit, when M = N and λ = 0, S becomes the identify matrix 1 and no smoothing is applied at all. 1 Because Φ is square when M = N , so that (Φ T Φ) -1 = Φ -1 (Φ T ) -1 . ","6 CHAPTER 1. SMOOTHING AND INTERPOLATION (a) M = 28 and γ =0 (b) M = 7 and γ =0 (c) M = 28 and γ = 1000 Figure 1.3: Smoothing of a 1D signal when the number of basis functions (M ) and the regularization strength (γ ) is varied. The regularizer is of the form Ω = ΓΦ, where Γ computes second-order finite differences (the n-th row has all zeros except for columns n - 1, n and n + 1, who have elements -1, 2 and -1, respectively). From top to bottom: basis functions that were used; noisy signal t and denoised signal ˆ t in blue and red, respectively; smoothing matrix S; and the middle row of S. ","1.3. SMOOTHING AND INTERPOLATION IN HIGHER DIMENSIONS 7 1.2.2 Interpolation When several medical images need to be compared, a common problem is that the intensities of an image need to be evaluated at locations x different from the integer locations x 1 =0,x 2 =1,...x N = N -1 where the intensities t 1 ,t 2 ,...,t N are defined. A standard solution is to set λ = 0, and use the shifted B-spline basis functions of (1.8) with scaling factor h = 1, i.e., a B-spline β p is centered around each of the N integer coordinates x n . Because M = N , the solution is given by 1 w = Φ -1 t, for which fast, dedicated numerical solvers are available [1]. As already analyzed above, the resulting function y(x, w) will pass exactly through the original intensities t n at the integer locations x n . However, the model will also fill in “interpolating” intensity values everywhere else, with the exact behavior depending on the order of the B-spline. For order p = 0, so- called nearest neighbor interpolation is obtained, in which the predicted value at location x is simply the value t n of the measurement location x n that is nearest to x 2 . For p = 1, linear interpolation is obtained, in which y(x, w) is piece-wise linear between the integer locations {x n } N n=1 . Finally, p = 3 results in so-called cubic interpolation, which is the method of choice in most practical applications. The effect of the B-spline order on the interpolation behavior of the model is illustrated in Fig. 1.4. 1.3 Smoothing and interpolation in higher di- mensions When going to D = 2 dimensions, the spatial locations x =(x 1 ,x 2 ) consist of two coordinates x 1 and x 2 , and the measurement signal becomes a two- dimensional image, represented by a matrix with N 1 rows and N 2 columns: T =      t 1,1 t 1,2 ··· t 1,N2 t 2,1 t 2,2 ··· t 2,N2 . . . . . . . . . . . . t N1,1 t N1,2 ··· t N1,N2      . It will be convenient to define a vectorization operation – denoted by vec(·)– that re-arranges this N 1 × N 2 image into a 1D signal t of length N = N 1 N 2 , by 2 Strictly speaking an exception is made at locations x that fall exactly half-way between two integer locations, where the average value is returned. ","8 CHAPTER 1. SMOOTHING AND INTERPOLATION Figure 1.4: Nearest-neighbor, linear and cubic interpolation. stacking the columns of T under each other: t = vec(T)=                t 1,1 . . . t N1,1 t 1,2 . . . t N1,2 . . . t N1,N2                . (1.10) In this re-arrangement, element t n1,n2 in T corresponds to element t n in t, where n = n 1 +(n 2 - 1)N 1 . In order to proceed, we also need to choose appropriate basis functions φ m (x) that work in 2D. A convenient choice is often separable basis functions, which are simply the product of two 1D basis functions (one taking as input x 1 , and the other one x 2 ). If there are M 1 basis functions in the first (row) direction, and M 2 in the second (column) direction, taking all the combinations yields a total of M = M 1 M 2 basis functions in 2D, given by φ m (x)= φ m1 (x 1 )φ m2 (x 2 ) (1.11) ","1.3. SMOOTHING AND INTERPOLATION IN HIGHER DIMENSIONS 9 Figure 1.5: The 36 separable 2D basis functions produced from two sets of 6 1D basis functions. for m = m 1 + m 2 M 1 (recall that m 1 and m 2 run from 0 to M 1 - 1 and M 2 - 1 respectively). Fig. 1.5 illustrates the 2D basis functions generated from 1D B-spline basis functions this way. To simplify notation, it will be convenient to use the Kronecker product of two matrices: A ⊗ B =    a 1,1 B a 2,1 B ... a 2,1 B a 2,2 B ... . . . . . . . . .    . If Φ 1 denotes the N 1 ×M 1 matrix containing the output of the M 1 basis functions evaluated at the integer locations x 1 =0,...,N 1 - 1, and Φ 2 is similarly defined for the second direction, then Φ = Φ 2 ⊗ Φ 1 (1.12) is a N × M matrix that contains vectorized versions of all M basis functions in 2D, where the m-th column contains (1.11) evaluated at all the pixel locations in T. Equipped with (1.10), (1.11) and (1.12), any smoothing or interpolation problem in 2D can be directly mapped into a 1D problem, so that the solutions described in Sec. 1.2 can (in principle) be directly applied. The extension to 3D is also straightforward; for example, vectorized versions of the output of M = M 1 M 2 M 3 basis functions, evaluated at all N = N 1 N 2 N 3 locations, are given by Φ = Φ 3 ⊗ Φ 2 ⊗ Φ 1 . ","10 CHAPTER 1. SMOOTHING AND INTERPOLATION 1.3.1 Exploiting separability Although the problem is in theory solved, in practice the computations involve storing and inverting a M × M matrix, which can be problematic when M is very large (i.e., when the model involves many basis functions). As an example, consider the interpolation of a 3D volume of size 256 × 256 × 256, an application in which there are M = 256 3 basis functions. At this size, naively storing a matrix with M 2 elements at 64 bits per element (double-precision floating point) would take 2048 TB; subsequently inverting it would take O(M 3 )= O(256 9 ) operations, making it entirely infeasible. In contrast, re-arranging the computations as outlined below only requires storing and inverting three 256 × 256 matrices, taking 512 KB and O(256 3 ) operations each. For a 2D image of size 256 × 256, the savings are more modest but still very substantial: storing a matrix of 32 GB vs. two matrices of 512 KB, and O(256 6 ) operations vs. two times O(256 3 ) operations for matrix inversions, which is almost 10 million times faster. When no regularization is used (λ = 0), the separability of the basis functions can be exploited as follows. In 2D, element m = m 1 + m 2 M 1 in the length-M vector c = Φ T t is given by c m = N X n=1 φ n,m t n = N1 X n1=1 N2 X n2=1 φ 1 n1,m1 φ 2 n2,m2 t n1,n2 = N1 X n1=1 φ 1 n1,m1  N2 X n2=1 φ 2 n2,m2 t n1,n2 ! , where φ 1 ·,· and φ 2 ·,· denote elements in Φ 1 and Φ 2 , respectively. The key insight is that the same summation over the second direction (in parentheses) is needed for each new m 1 , and therefore needs to be computed only once. In matrix form, this can be expressed as C = Φ T 1 TΦ 2 , where C is a M 1 × M 2 matrix such that vec(C)= c. Similarly, it can be shown that Φw can be more efficiently computed as Φ 1 WΦ T 2 , with vec(W)= w. Therefore, the solution (1.3), which can be written as Φ T (Φw)= Φ T t, can also be expressed as Φ T 1  Φ 1 WΦ T 2  Φ 2 = Φ T 1 TΦ 2 , so that finally W =  Φ T 1 Φ 1  -1 Φ T 1 TΦ 2  Φ T 2 Φ 2  -1 (1.13) can be used to compute the elements of w very efficiently. ","1.3. SMOOTHING AND INTERPOLATION IN HIGHER DIMENSIONS 11 1.3.2 Smoothing in 2D Plugging the solution (1.13) in (1.9), and using the same approach as above, it is easy to see that a smoothed image can be obtained as ˆ T = S 1 TS T 2 , where S 1 = Φ 1  Φ T 1 Φ 1  -1 Φ T 1 is a N 1 × N 1 smoothing matrix that smoothes each column in T independently in the row-direction only, and the corresponding S 2 subsequently smoothes each row in the column direction (or vice versa). This is illustrated in Fig. 1.6. 1.3.3 Interpolation in 2D For interpolation, the parameters are simply obtained as W = Φ -1 1 T ( Φ -1 2 ) T . Interpolated values at new locations x are then computed using (1.1), exploiting the separability of the basis functions (1.11). Nearest-neighbor, linear and cubic interpolation in 2D are illustrated in Fig. 1.7. ","12 CHAPTER 1. SMOOTHING AND INTERPOLATION (a) T (b) ˆ T (c) S1T (d) TS T 2 Figure 1.6: Smoothing of a 2D image T by fitting the 2D basis functions of Fig. 1.5 to it. Computationally the smoothed result ˆ T can be obtained by smoothing across the rows and then the columns (or vice versa), using the 1D basis functions shown in Fig. 1.5. ","1.3. SMOOTHING AND INTERPOLATION IN HIGHER DIMENSIONS 13 (a) (b) (c) (d) Figure 1.7: 2D nearest-neighbor (b), linear (c) and cubic (d) interpolation within the small image area indicated in (a). ","14 CHAPTER 1. SMOOTHING AND INTERPOLATION ","Chapter 2 Image Registration In many situations, the information contained in two or more images needs to be combined. Examples of such situations include interpreting images of the same patient acquired at different time points, or with different imaging modalities, as well as comparing the anatomy or function between various subject groups (e.g., to study how patients differ from controls in a clinical research study). In order for images to be used this way, they need to be spatially aligned so that corresponding structures appear in corresponding locations. The process of aligning images is called image registration. This chapter introduces some of its basic concepts in a medical imaging context. 2.1 Coordinate systems So far we have taken a rather cavalier attitude regarding the spatial locations of voxels (pixels in 3D): for 3D images we have simply assumed that we can index individual voxels by their location x =(x 1 ,x 2 ,x 3 ) T , where x 1 , x 2 and x 3 are integers. In reality, however, scanners can generate images with almost any voxel size (spacing between the voxels in each of the three dimensions) and in any orientation. In magnetic resonance imaging (MRI), for instance, it is not uncommon to acquire multiple images when a patient is inside the scanner, each defined on its own image grid. An example is shown in Fig. 2.1. In order to relate multiple images to each other, we’ll need to differentiate the concept of “voxel coordinates” (which are defined in integers units, and index individual voxels in a 3D image grid) from that of “world coordinates” (indicating the spatial positions of voxels in the real world, and measured in mm). Specifically, let v =(v 1 ,v 2 ,v 3 ) T denote the voxel coordinate of a voxel in an image of size N 1 × N 2 × N 3 , where v d =0,...N d - 1 for all dimensions d =1,..., 3. In addition to a N 1 × N 2 × N 3 array of intensities, the scanner will also encode a 3 × 3 matrix A and a 3 × 1 vector t to map voxel coordinates into world coordinates as follows: x = Av + t, (2.1) 15 ","16 CHAPTER 2. IMAGE REGISTRATION (a) Displayed in world coordinates (b) Displayed in voxel coordinates Figure 2.1: Two MR images acquired within the same scan session: T1-weighted (left) and T2-weighted (right). Although the two images show corresponding structures in corresponding locations in world coordinates (a), the 3D array of image intensities acquired by the scanner is actually very different between the two scans: The T1-weighted scan is a 256 × 256 × 150 volume with voxel size 0.94 × 0.94 × 1.2 mm 3 acquired in the sagittal direction, whereas T2-weighted scan is a 256 × 256 × 28 volume with voxel size 0.90 × 0.90 × 4.98 mm 3 acquired in the axial direction. This is clearly visible when displaying the two images in voxel coordinates instead (b). ","2.1. COORDINATE SYSTEMS 17 where A =   a 1,1 a 1,2 a 1,3 a 2,1 a 2,2 a 3,3 a 3,1 a 3,2 a 3,3   and t =   t 1 t 2 t 3   . (2.2) For instance, the combination A =   0.95 0 0 0 0.95 0 0 0 4.5   and t =   -120.0 -120.0 -65.0   for an image of size 256 × 256 × 30 would indicate that the size of a voxel is 0.95mm × 0.95mm × 4.5 mm, and that the origin of the world coordinate system (the location where x = 0) lies somewhere in the middle of the image array. Very often, A will also include more general spatial transformations, such as 3D rotations or “flipping” of axes (negative voxel spacings, so that an increase in some voxel coordinate(s) will results in a decrease in world coordinate(s)). Who decides what directions A should encode, or where the origin of the world coordinate system should be located? This is a matter of convention, and several such conventions exist. The origin is often considered to be approxi- mately at the center of the anatomical structure being scanned; a well-known world coordinate system is the RAS convention, where x 1 increases towards the R ight of the patient, x 2 towards the A nterior (front), and x 3 towards the S uperior (top) of the patient. Another often-used convention is LPS (L eft, P osterior, S uperior), which is similar to the RAS system but with the direction of the first two axes swapped. When working with medical images, it is criti- cally important to know what convention is used for each image, lest a patient be operated on the wrong side of their body! Consider again the situation in Fig. 2.1, where two different images of the same patient were acquired within the same scanning session: one T1-weighted scan with voxel-to-world mapping {A T 1 , t T 1 }, and one T2-weighted scan with {A T 2 , t T 2 }. In order to compute the voxel coordinate v T 2 in the T2-weighted scan corresponding to a voxel coordinate v T 1 in the T1-weighed one, it is often convenient to re-write (2.1) as follows:     x 1 x 2 x 3 1     =     a 1,1 a 1,2 a 1,3 t 1 a 2,1 a 2,2 a 2,3 t 2 a 3,1 a 3,2 a 3,3 t 3 0 0 0 1     | {z } M     v 1 v 2 v 3 1     , (2.3) where the 4 × 4 matrix M is called an affine matrix. This technique, which uses so-called homogeneous coordinates (vectors are augmented with a 1 at the end), absorbs the addition of the vector t into a single matrix multiplication, which makes concatenating several affine matrices very easy: the mapping of v T 1 to v T 2 is given by:  v T 2 1  = M -1 T 2 · M T 1 ·  v T 1 1  . ","18 CHAPTER 2. IMAGE REGISTRATION We will soon see examples where more than two affine matrices are combined this way. 2.2 Transformation models In many situations, images will not be perfectly aligned as in Fig. 2.1. This could be because the patient has moved between the two acquisitions, for instance because the scanning was performed on a different day or on a different scanner (e.g., MRI vs. CT scan). It could also be because of organ deformation, caused by breathing in a dynamic MRI, for instance, or because of tumor shrinkage or patient weight loss during the course of a radiation therapy treatment. In such situations, an additional geometrical transformation exists that a registration method should try to recover so that the images can still be compared to each other. Let x =(x 1 ,...,x D ) T denote a spatial location (in world coordinates) in an image that we will call the fixed image in the remainder, where D = 2 in 2D and D = 3 in 3D images. Similarly, we will use y =(y 1 ,...,y D ) T to denote spatial locations (again in world coordinates) in another image, which we will refer to as the moving image. Our task is to find a transformation model y(x, w)=    y 1 (x, w) . . . y D (x, w)    (2.4) so that points in the fixed image are mapped to the corresponding anatomical locations in the moving image. Here y d (x, w) is a function that governs how points in the fixed image move along the d-th direction in the moving image as the parameter vector w is varied. 2.2.1 Linear transformations Affine transformation: In the most general form of linear transformations, lines that are straight in the fixed image will still be straight when mapped into the moving image, but angles and areas/volumes are typically not preserved. An illustration is provided in Fig. 2.2a. Such transformations are useful to roughly align images of different individuals or time points (for instance as a first step in a subsequent nonlinear registration algorithm), so that the major differences in size and orientation are removed. The transformation model is governed by: y(x, w)= Ax + t, where A and t are given by A =  a 1,1 a 1,2 a 2,1 a 2,2  and t =  t 1 t 2  ","2.2. TRANSFORMATION MODELS 19 in 2D, and by (2.2) in 3D. Here t implements a translation, whereas rotation, scaling and skewing are encoded in A. This transformation model is called the affine transformation model. Referring to (2.4), the d-th coordinate of points mapped into the moving image is therefore given by the linear function y d (x, w d )= t d + a d,1 x 1 + ... + a d,D x D , (2.5) with parameters w d =(t d ,a d,1 ,...,a d,D ) T . Note that these parameters only contain elements of the d-th row of A and t; the vector w =(w T 1 ,..., w T D ) T collects all the transformation parameters of the entire D-dimensional model. In 3D, the affine matrix notation of (2.3) can be used to compute the map- ping of a voxel coordinate v F in the fixed image into the corresponding voxel coordinate v M in the moving image as follows: v M = M -1 M · M · M F · v F , (2.6) where M F and M M denote the voxel-to-world affine matrix of the fixed and the moving image, respectively. Rigid transformation: In situations where the moving and the fixed image were both taken from the same rigid structure (e.g, the head) in the same subject, the ability of the affine transformation model to arbitrarily scale and skew images becomes a liability. In those cases, a model that can only account for a translation and a rotation is more appropriate: y(x, w)= Rx + t, where R T R = I and det(R) = 1. This transformation model is called the rigid transformation model, and is illustrated in Fig. 2.2b. In 2D, rotations can be enforced by parameterizing the rotation matrix R as follows: R =  cos(α) - sin(α) sin(α) cos(α)  , where α is a rotation angle. The parameters of this model are therefore w = (α, t 1 ,t 2 ) T . In 3D there are many possible parametrizations of the rotation matrix, but one possibility is as follows: R =   1 0 0 0 cos(γ) - sin(γ) 0 sin(γ) cos(γ)   ·   cos(β) 0 - sin(β) 0 1 0 sin(β) 0 cos(β)   ·   cos(α) - sin(α) 0 sin(α) cos(α) 0 0 0 1   , where α, β and γ are three rotation angles; the parameters of the model are then given by w =(α,β,γ,t 1 ,t 2 ,t 3 ) T . Provided the affine matrix M is constructed by using A = R,(2.6) remains valid to map voxel coordinates in the fixed image into voxel coordinates in the moving image. ","20 CHAPTER 2. IMAGE REGISTRATION (a) Affine transformation (b) Rigid transformation (c) Nonlinear transformation Figure 2.2: Illustration of affine, rigid and nonlinear transformation models. The red grid shows how the regular grid shown in blue is characteristically mapped under each transformation model. ","2.3. LANDMARK-BASED REGISTRATION 21 2.2.2 Nonlinear transformations In situations where tissue deformation needs to be modeled, the linear func- tion (2.5) (one for each dimension d) in affine registration is generalized to a nonlinear one. Since global differences in overall size and orientation have typically already been removed using a preceding affine registration 1 , only the residual deformation δ d is modeled: y d (x, w d )= x d + δ d (x, w d ), where δ d (x, w d )= M-1 X m=0 w d,m φ m (x). (2.7) Here φ m (x) are M basis functions, which are typically taken to be separable (cf. (1.11) and Fig. 1.5), with weights w d =(w d,0 ,...w d,M-1 ) T . The interpre- tation of these weights is that, when they are all set to zero (i.e., when w d = 0), the deformation δ d is also zero and no deformation is applied. An illustration of a nonlinear transformation encoded this way is provided in Fig. 2.2c. It is worth reiterating that the motion of points along each individual di- mension d in the moving image is governed by its own set of parameters w d . In 2D, there will therefore by two sets of parameters: w =(w 1 , w 2 ) T , whereas in 3D there will be three: w =(w 1 , w 2 , w 3 ) T . 2.3 Landmark-based registration One way to register two images is by manually annotating corresponding points in both images, and then finding a spatial transformation that brings matching point pairs close to each other. Letting {x n } N n=1 denote a set of N point locations annotated in the fixed image, and {y n } N n=1 the corresponding locations in the moving image, registration can be obtained by minimizing the energy E(w)= N X n=1 ky n - y(x n , w)k 2 with respect to the transformation parameters w. Here ka - bk 2 = ∑ D d=1 (a d - b d ) 2 measures the squared Euclidean distance between two points a and b. Affine transformation: Optimizing E(w) with respect to w will be partic- ularly straightforward when the mapping in each dimension d is governed by its own set of parameters w d , as is the case in both the affine and nonlinear formulation of (2.5) and (2.7). This is because the energy can then be split into 1 For instance by replacing the affine voxel-to-world mapping of the fixed image M F by M · M F ","22 CHAPTER 2. IMAGE REGISTRATION a sum of D independent energies, one for each dimension: E(w) = N X n=1 D X d=1 [y n,d - y d (x n , w d )] 2 = D X d=1 E d (w d ) with E d (w d )= N X n=1 [y n,d - y d (x n , w d )] 2 , where y n,d denotes the d-th element of y n . Optimizing each E d (w d ) with respect to w d is easy: in both the affine and the nonlinear case, y d (x n , w d ) is only linearly dependent on w d , reducing the problem to the form of linear regression analyzed in Sec. 1.1. Taking affine registration as an example, the energy for the d-th dimension is given by (cf. (2.5)) E d (w d )= N X n=1 [y n,d - t d - a d,1 x n,1 - ... - a d,D x n,D ] 2 , which is minimized at solution      t d a d,1 . . . a d,D      = ( X T X ) -1 X T    y 1,d . . . y N,d    , (2.8) where X =      1 x 1,1 ··· x 1,D 1 x 2,1 ··· x 2,D . . . . . . . . . . . . 1 x N,1 ··· x N,D      . The d-th row of the affine transformation parameters A and t is therefore given by (2.8). Doing this for all D dimensions yields all the required parameter values. Rigid transformation: Optimizing E(w) with respect to w is more involved when a rigid transformation model is used, since the constraints that R T R = I and det(R) = 1 prevent us from decoupling the problem across dimensions. For the translation vector t this is not yet an issue, and we can therefore use the same approach as before to deduce that, for a given rotation R, the energy E(w)= N X n=1 ky n - Rx n - tk 2 (2.9) is minimized when t = ¯ y - R¯ x, (2.10) ","2.4. INTENSITY-BASED REGISTRATION 23 where ¯ y = 1 N ∑ N n=1 y n and ¯ x = 1 N ∑ N n=1 x n . Plugging this result into (2.9), we can reformulate the energy as E(w)= N X n=1 k˜ y n - R˜ x n k 2 where ˜ y n = y n - ¯ y and ˜ x n = x n -¯ x. Under the constraint R T R = I, this is minimized when [2] R = VU T , where U and V are D × D matrices such that U T U = I, V T V = I and N X n=1 ˜ x n ˜ y T n = UΣV T , where Σ is diagonal. One such a solution is obtained by computing the singular value decomposition (SVD) of the matrix ∑ N n=1 ˜ x n ˜ y T n . However, this solution does not necessarily satisfy the second constraint of rotational matrices that det(R) = 1. It is also possible 2 that det(R)= -1, in which case a valid rotation can be obtained by “flipping” one of the columns of R by reversing the sign of all the elements in it. Once R has been found, (2.10) can be used to find the remaining parameters t. 2.4 Intensity-based registration Landmark-based registration suffers from a number of shortcomings, including the need for manually annotating images and the fact that registrations are computed from only a handful of points. This limits both the efficiency and the accuracy with which images can be aligned. Another class of algorithms can perform registrations fully automatically, by minimizing energies that are computed directly from raw image intensities. Below we review two well-known methods in this family. 2.4.1 Sum of squared differences When the fixed and the moving image both depict the same anatomical struc- tures with the same (or very similar) intensity characteristics, a successful reg- istration will be characterized by small differences in absolute intensity values at corresponding locations. Scenarios where this idea can be used include co- registering two CT scans, where intensity values have a direct physical inter- pretation, or two MR images acquired with similar pulse sequences on similar hardware (e.g., T1-weighted images acquired at different time points or from dif- ferent subjects on the same scanner). In the latter case, images will often still 2 Since det(AB) = det(A) det(B), we have that det(R) = det(U) det(V). Furthermore, det(U) ± 1 and det(V) ± 1 since U T U = I and V T V = I. ","24 CHAPTER 2. IMAGE REGISTRATION need to be pre-processed to make their intensities comparable (for instance by intensity rescaling), since MR scanners do not typically provide measurements in quantitative physical units. On a technical level, registration can be obtained by minimizing the sum of squared differences in intensities at corresponding locations: E(w)= N X n=1 [F (x n ) -M(y(x n , w))] 2 . (2.11) Here, x n are the world coordinates of the voxels in the fixed image, and F (x n ) denotes the image intensity of those voxels. Similarly, M(y(x n , w)) denotes the image intensities in the moving image, evaluated at the mapped locations y(x n , w). Since these will generally fall in between original voxel locations in the moving image, image interpolation will be required (cf. Sec. 1.3.3). When the mapped location falls outside of the image area of the moving image, a constant intensity will typically be assigned (for instance zero). Gauss-Newton optimization Unlike in landmark-based registration, finding parameter values w that mini- mize the energy (2.11) is no longer given by closed-form solutions, and numer- ical optimization methods need to be used. When the number of parameters of the transformation model is low, for instance in rigid or affine registration, generic optimization algorithms that only evaluate E(w) (and perhaps its gra- dient ∇E(w)) will work quite well. For cases with many degrees of freedom, such as flexible nonlinear deformations, a dedicated optimization “trick” can be used that exploits the specific structure of the energy (2.11) to obtain faster solutions. The method is known as Gauss-Newton, and involves linearizing M(y(x n , w)) with respect to w around the current parameter values. Specifically, for the non- linear transformation model of (2.7), where deformations are encoded in each dimension d separately, the partial derivative of M(y(x n , w)) with respect to w d,m is given by (chain rule): ∂ M(y(x n , w)) ∂w d,m = ∂ M(y(x n , w)) ∂y d ∂y d (x n , w d ) ∂w d,m = g d,n φ m (x n ). (2.12) Here the fact that ∂y d (xn,w d ) ∂w d,m = φ m (x n ) was used, and the notation g d,n = ∂M(y(xn,w)) ∂y d was introduced as shorthand for the partial spatial derivative of the moving image in the d-th dimension, evaluated at position y(x n , w). By using a cubic B-spline interpolation model, such spatial derivatives can be conveniently computed everywhere [3]. For a small deviation  from some parameter values w,(2.12) can now be used in a first-order Taylor expansion to obtain the approximation M(y(x n , w + )) ’M(y(x n , w)) + D X d=1 M X m=0 ( g d,n φ m (x n ) )  d,m . (2.13) ","2.4. INTENSITY-BASED REGISTRATION 25 Plugging this into the energy (2.11), and defining τ n = F (x n ) -M(y(x n , w)) we obtain E(w + ) ’ N X n=1 \" τ n - D X d=1 M X m=0 ( g d,n φ m (x n ) )  d,m # 2 , which can be recognized has having the form of a linear regression problem with parameters . The value of  minimizing E(w + ) is therefore given by (cf. Sec. 1.1)  =  Ψ T Ψ  -1 Ψ T τ (2.14) where τ =(τ 1 ,...,τ N ) T and Ψ = ( G 1 Φ ··· G D Φ ) , where G d = diag(g d,n ,...,g d,N ) and Φ is given by (1.2). This result suggests the following simple iterative algorithm for optimizing E(w): 1. Select a starting value for the transformation parameters, e.g., w = 0 (no deformation); 2. Use (2.14) to compute a small update  to the current parameters w; 3. Update the parameters: w ← w + ; 4. Repeat steps 2. and 3. until convergence is detected. There is one catch, though: In the derivation above, we have assumed that  is “small”. What happens if we compute  and one or more of its components are actually quite large? Then our approximation (2.13) will have been a poor one, and we might find that the energy E(w) increases (instead of decreasing ) after we use  to update w. To avoid such situations, several methods to modify  exist. One such a modification is the Levenberg-Marquardt algorithm, which replaces (2.14) by  =  Ψ T Ψ + λI  -1 Ψ T τ , (2.15) where λ ≥ 0 is a tunable parameter that is typically updated after each iteration. If the parameter update  does not decrease E(w), the update is rejected and λ is increased until a decrease in E(w) is obtained; this will happen eventually because for very large λ the algorithm devolves into a gradient-descent algorithm with a small step size. Conversely, if E(w) decreases a smaller λ is used in the next iteration of the algorithm to improve efficiency. ","26 CHAPTER 2. IMAGE REGISTRATION 2.4.2 Mutual Information When two images need to be aligned that were acquired with two different imag- ing modalities (e.g., CT vs. MR, or MR vs. PET), the intensity characteristics of most anatomical structures will generally be quite different between the two images. This precludes the use of the sum-of-squared-differences energy as a registration criterion. In such situations, fully automatic registrations can still be obtained by optimizing the Mutual Information (MI) between the two images [4, 5]. For given transformation parameter values w, the frequency with which specific discretized intensity pairs (f,m) occur is analyzed. Here, f ∈{1,...,B} and m ∈{1,...,B} denote intensities in the fixed and moving image, respectively, after both images have been preprocessed to only contain B discrete intensity levels 3 . More specifically, a joint histogram H =    h 1,1 ... h 1,B . . . . . . . . . h B,1 ... h B,B    is computed, where entry h f,m contains the number of times the intensity in the fixed image was f , while the corresponding intensity in the moving image was m. In practice, this is done by starting with an empty joint histogram (H = 0), and filling it up by looping over all N voxels in the fixed image. For each such voxel n, its intensity F (x n ) as well as the corresponding intensity in the moving image M(y(x n , w)) is determined. The former will directly give us a value for f , but deciding on m is more involved since y(x n , w) will typically fall somewhere between the voxels in the moving image. An easy solution is to use nearest- neighbor interpolation, so that m is simply the intensity of the voxel that is closest to y(x n , w); however in practice more involved schemes are typically used. A related issue is what to do if y(x n , w) maps to a location outside of the image area of the moving image. In many implementations such voxels are simply skipped (i.e., they do not contribute to the joint histogram). For most voxels, though, a valid intensity pair (f,m) is obtained; the corresponding joint histogram count h f,m is then increased by one, and the next voxel in the fixed image is visited. An example of a joint histogram computed this way is shown in Fig. 2.3e. A key insight for registration purposes is that the joint histogram will typi- cally have many entries with small counts when the two images are well aligned: most of the encountered intensity combinations (f,m) will be concentrated in just a few histogram bins. In contrast, when the images are moved out of align- ment, intensity pairs will become more variable, “smearing out” the counts from high-count histogram entries into (what were previously) low-count bins. This process is illustrated in Fig. 2.4. This phenomenon can be exploited to define 3 Typically this is done by dividing the intensity range in each image into B contiguous intervals, and recording the interval (“bin”) number of each voxel’s intensity. Often-used values for B are 32, 64 or 128. ","2.4. INTENSITY-BASED REGISTRATION 27 (a) MR (b) CT (c) Marginal histogram (MR) (d) Marginal histogram (CT) (e) Joint histogram Figure 2.3: An MR and CT image in perfect alignment, along with their nor- malized (i.e., divided by N ) joint and marginal histograms. The number of histogram bins was B = 32. ","28 CHAPTER 2. IMAGE REGISTRATION the following registration energy: E(w)= H F,M with H F,M = - B X f =1 B X m=1 p f,m log(p f,m ), where p f,m = h f,m /N are normalized histogram counts. These can be inter- preted as probabilities of seeing specific intensity combinations (f,m) when the images are aligned with parameter value w. The quantity H F,M is known as the joint entropy in information theory. It measures how predictable intensity combinations (f,m) are, and is directly related to data compression: The more predictable the intensity combinations, the lower the joint entropy and the fewer bits will theoretically be needed to store or communicate the fixed-moving im- age pair. Applied to our registration setting, the joint entropy will be higher when the joint histogram is “smeared out”, which will happen when the images are not well aligned. This explains why H F,M can be used as an energy function to drive an automatic registration process. One problem with using the joint entropy is that it attains low values not just when registrations are good, but also when some non-desirable trivial solutions are found. As an example, consider the scenario where w is so that the fixed image and the moving image only overlap in a region in the background. In most implementations, the joint histogram is only computed from this overlapping region, as explained before, and therefore the only intensity pair with a non- zero count will be the one where both f and m are zero. The joint entropy for this solution will be zero, which is its lowest possible value. In order to avoid such pathological solutions, it is customary to alter the energy as follows: E(w)= H F,M - H F - H M , (2.16) where H F and H M are the marginal entropies of the fixed and moving image, respectively. They are defined as follows: H F = - B X f =1 p f log(p f ) and H M = - B X m=1 p m log(p m ), where p f = ∑ B m=1 p f,m denotes the probability of encountering a voxel with intensity f in the fixed image, and p m = ∑ B f =1 p f,m the corresponding proba- bility of encountering intensity m in the moving image. Adding these two terms to the energy function will steer the registration towards overlapping in areas that are “interesting” in both images, i.e., areas with actual content (with high marginal entropy). Since the quantity H F + H M - H F,M is known in information theory as mutual information, using its negative as energy function in (2.16) for registra- tion purposes is known as mutual information-based registration. In a practical implementation, numerical optimization will need to be used to find values w with low energy. Currently these are mostly general-purpose optimizers that only need to be able to evaluate E(w) and its gradient ∇E(w). ","2.4. INTENSITY-BASED REGISTRATION 29 (a) (b) Figure 2.4: Normalized joint histogram of the MR and CT scans shown in Fig. 2.3, both when the images are in perfect alignment (a) and when the CT image is translated with respect to the MR image (b). ","30 CHAPTER 2. IMAGE REGISTRATION ","Chapter 3 Model-based Segmentation The ability to efficiently delineate anatomical structures from medical images is important in many applications. Examples include measuring the number and volume of lesions and how they change over time, planning radiation therapy treatments or surgical interventions, and characterizing shape changes that oc- cur in specific patient groups. The process of delineating structures is called image segmentation, and automating it has traditionally been approached using model-based techniques. This chapter reviews some well-established techniques in this category. 3.1 Generative models Image segmentation methods are often based on so-called generative models, i.e., models that describe how images can be generated synthetically by random sampling from some probability distribution. Generative models for medical image segmentation generally consist of two parts: -A prior distribution that makes predictions about where anatomical struc- tures typically occur throughout the image. We will refer to this compo- nent of the model as the labeling model. Let l =(l 1 ,...,l N ) T be a (vector- ized) label image with a total of N voxels, with l n ∈{1,...,K} denoting the one of K possible labels assigned to voxel n, indicating which of the K anatomical structures the voxel belongs to. The labeling model is then specified by some probability distribution p(l|θ l ) that typically depends on a set of parameters θ l . -A likelihood function that predicts how any given label image, where each voxel is assigned a unique anatomical label, translates into an image where each voxel has an intensity. Because this really is a (often very simplis- tic) model of how a medical imaging device generates images from known anatomy, we will refer to this component of the model as the imaging 31 ","32 CHAPTER 3. MODEL-BASED SEGMENTATION model. Given a label image l, the imaging model generates a correspond- ing intensity image d =(d 1 ,...,d N ) T by randomly sampling from some probability distribution p(d|l, θ d ) with parameters θ d . In summary, the generative model is fully specified by two parametric distri- butions p(l|θ l ) and p(d|l, θ d ), which depend on parameters θ =(θ T l , θ T d ) T that are either assumed to be known in advance, or more frequently, need to be es- timated from the image data itself. The exact form of the used distributions depends on the segmentation problem at hand. In general, the more realistic the models, the better the segmentations that can be obtained with them. Once the exact generative model has been chosen and appropriate values ˆ θ for its parameters are known, properties of the underlying segmentation of an image can be inferred by inspecting the posterior probability distribution p(l|d, ˆ θ). Using Bayes’ rule, this distribution is given by p(l|d, ˆ θ)= p(d|l, ˆ θ d )p(l| ˆ θ l ) p(d| ˆ θ) , (3.1) with p(d| ˆ θ)= ∑ l p(d|l, ˆ θ d )p(l| ˆ θ l ) 1 . For instance, one might look for the seg- mentation ˆ l that has the maximum a posteriori (MAP) probability: ˆ l = arg max l p(l|d, ˆ θ), (3.2) or estimate the volume of the anatomical structure corresponding to label k by assessing it’s expected value X l V k (l)p(l|d, ˆ θ) (3.3) where V k (l) counts the number of voxels that have label k in l. 3.2 Gaussian mixture model A very simple generative model that is nevertheless quite useful in practice, is the so-called Gaussian mixture model. In this model, the segmentation prior is of the form p(l|θ l )= N Y n=1 p(l n |θ l ) (3.4) with p(l = k|θ l )= π k , (3.5) where the parameters θ l =(π 1 ,...,π K ) T consist of a set of probabilities π k satisfying π k ≥ 0, ∀k and ∑ K k=1 π k = 1. In other words, this model assumes 1 In practice, one seldom needs to explicitly calculate the denominator p(d| ˆ θ) be- cause it doesn’t involve l, and one simply compares alternate segmentations by evaluating p(d|l, ˆ θ d )p(l| ˆ θ l ) instead. ","3.2. GAUSSIAN MIXTURE MODEL 33 that the labels are assigned to the voxels independently from one another, i.e., the probability that a certain label occurs in a particular voxel is unaffected by the labels assigned to other voxels ((3.4)), and each label occurs, on average, with a relative frequency of π k ((3.5)). For the likelihood function, it is assumed that the intensity in each voxel only depends on the label in that voxel and not on that in other voxels: p(d|l, θ d )= N Y n=1 p(d n |l n , θ d ), (3.6) and that the intensity distribution associated with each label k is Gaussian with mean μ k and variance σ 2 k : p(d|l = k, θ d )= N (d|μ k ,σ 2 k ), (3.7) where N (d|μ, σ 2 )= 1 √ 2πσ 2 exp  - (d - μ) 2 2σ 2  (3.8) and θ d =(μ 1 ,...,μ K ,σ 2 1 ,...σ 2 K ) T . It is instructive to write down the probability with which this model gener- ates a given image d: p(d|θ) = X l p(d|l, θ d )p(l|θ l ) = X l \" N Y n=1 N (d n |μ ln ,σ 2 ln ) N Y n=1 π ln # (3.9) = N Y n=1 p(d n |θ) (3.10) with p(d|θ)= K X k=1 N (d|μ k ,σ 2 k )π k . (3.11) (Although the transition from (3.9) to (3.10) may appear non-trivial, it is merely algebra and can easily be understood by considering that first the label and then the intensity is drawn independently in each individual voxel, hence the product over all voxels in (3.10).) (3.11) explains why this model is called the Gaussian mixture model: the intensity distribution in any voxel, independent of its spatial location, is given by the same linear superposition of Gaussians. Since no spatial information is encoded in the model, it can directly be visualized as a way to approximate the histogram, as shown in Fig. 3.1. Because of the assumption of statistical independence between voxels, the segmentation posterior (3.1) reduces to a simple form that is factorized (i.e., ","34 CHAPTER 3. MODEL-BASED SEGMENTATION (a) (b) Figure 3.1: In the Gaussian mixture model, the histogram is described as a linear superposition of Gaussian distributions: (a) MR scan of the head, after remov- ing all non-brain tissue and other pre-processing steps; and (b) corresponding histogram and its representation as a sum of Gaussians. ","3.2. GAUSSIAN MIXTURE MODEL 35 Figure 3.2: Visualization of the segmentation posterior corresponding to the data and model of Fig. 3.1. High intensities correspond to high probabilities and vice versa. appears as a product) over the voxels: p(l|d, ˆ θ) = p(d|l, ˆ θ d )p(l| ˆ θ l ) p(d| ˆ θ) = Q N n=1 N (d n | ˆ μ ln , ˆ σ 2 ln ) Q N n=1 ˆ π ln Q N n=1 ∑ K k=1 N (d n | ˆ μ k , ˆ σ 2 k )ˆ π k = N Y n=1 p(l n |d n , ˆ θ), (3.12) where p(l = k|d, ˆ θ)= N (d| ˆ μ k , ˆ σ 2 k )ˆ π k ∑ K k 0 =1 N (d| ˆ μ k 0 , ˆ σ 2 k 0 )ˆ π k 0 . (3.13) Therefore, the segmentation posterior is fully specified by each voxel’s K pos- terior probabilities of belonging to each structure; such segmentation posteriors can be visualized as images where high and low intensities correspond to high and low probabilities, respectively. The segmentation corresponding to the im- age and Gaussian mixture model of Fig. 3.1 is visualized in 3.2 this way. Evi- dently, the sum of all the structures’ posterior probabilities add to one in each voxel: ∑ K k=1 p(l n = k|d n , ˆ θ)=1, ∀n. Because of the factorized form of the segmentation posterior, the MAP seg- mentation (3.2) is simply given by ˆ l = arg max l p(l|d, ˆ θ) = arg max l1,...,l N N Y n=1 p(l n |d n , ˆ θ), (3.14) i.e., each voxel is assigned exclusively to the label with the highest posterior probability. Similarly, the expected volume of the anatomical structure corre- ","36 CHAPTER 3. MODEL-BASED SEGMENTATION sponding to label k is given by (3.3) X l V k (l)p(l|d, ˆ θ)= N X n=1 p(k|d n , ˆ θ), (3.15) i.e., a “soft” count of voxels belonging to the structure, where voxels contribute according to their posterior probability of belonging to that structure. 3.3 Markov random field priors It is worth emphasizing that in the Gaussian mixture model, a voxel’s posterior probability of belonging to each of the K structures is computed using only the local intensity of the voxel itself ((3.13)). Although this works quite well in some applications, there is often an intensity overlap between the to-be-segmented structures, causing severe segmentation errors in such a purely intensity-driven strategy. An example of this is shown in Fig. 3.3, where a simple Gaussian mixture model with K = 2 classes was used to segment a brain MR scan into tumor tissue and other structures. The parameters ˆ θ were obtained by manu- ally clicking on a set of representative voxels within the tumor, recording their intensities, and computing their mean and variance as estimates of ˆ μ 1 and ˆ σ 2 1 , respectively; the mean and variance { ˆ μ 2 ˆ σ 2 2 } for the “other” class was simply the mean and variance of all the image’s voxels’ intensities; and the tumor was estimated to cover approximately one tenth of the image area, so that we used ˆ π 1 =0.1 and ˆ π 2 =0.9. It can be seen from the figure that while this model generally captures the tumor quite well, many small image areas outside of the tumor also have a high posterior probability of belonging to the tumor class, limiting the usefulness of the results. In order to avoid this type of segmentation errors, we need to use more advanced models for the prior distribution p(l|θ l ) that more realistically reflect the shape of the structures we are looking for. 3.3.1 Markov random field model An often-used improvement to the simplistic prior of (3.5) is to use a prior that explicitly prefers voxels with the same label to be clustered spatially rather than scattered randomly throughout the image area. A computationally attractive way of achieving this is to formulate the prior as p(l|θ l )= 1 Z (θ l ) exp(-U (l|θ l )), (3.16) where U (l|θ l ) is an “energy” functional that is high for undesired configura- tions of l and low otherwise, resulting in low prior probabilities of undesired configurations and high probabilities otherwise. Z (θ l )= ∑ l exp(-U (l|θ l )) is a normalizing constant that ensures that ∑ l p(l|θ l ) = 1 but typically does not need to be computed explicitly in practical situations. ","3.3. MARKOV RANDOM FIELD PRIORS 37 (a) (b) Figure 3.3: The Gaussian mixture model does not encode any spatial infor- mation and is therefore susceptible to segmentation errors caused by intensity overlap between the structures of interest: (a) a brain MR scan of a person with a tumor; and (b) the voxels’ posterior probability of belonging to the tumor for a 2-component Gaussian mixture model. For reasons that will soon become clear, the energy functional is often chosen to be of the form U (l|θ l )= β X (n,n 0 ) δ(l n 6= l n 0 ), (3.17) where the sum is running over all the voxel pairs (n, n 0 ) that are neighbors in the image grid (for instance, each voxel has six direct neighbors in a 3-D image grid, or 26 if also the diagonal directions are allowed), δ(k 6= l) equals zero if k = l and one otherwise, and β is a parameter that controls how strongly undesired configurations are penalized. Stated differently, the energy functional is proportional to the number of times two neighboring voxels have a different class assignment in l, thereby encouraging configurations in which the labels are spatially clustered. Prior knowledge that some classes tend to occur more frequently than others can also be incorporated by adding an extra term: U (l|θ l )= β X (n,n 0 ) δ(l n 6= l n 0 ) - N X n=1 log(π ln ). (3.18) The parameters of this model are θ l =(β,π 1 ,...,π K ) T ; for β = 0 (no undesired pair-wise combination penalized) this model reduces to the standard Gaussian mixture prior of (3.5). The computational attractiveness of the model lies in the fact that, although it defines a global prior that induces statistical dependencies between labels in voxels that are far apart, calculating the conditional distribution in a single voxel requires only looking up the labels assigned to its neighboring voxels (so-called Markov property). Indeed, using the notation l \\n =(l 1 ,...,l n-1 ,l n+1 ,...,l n ) T ","38 CHAPTER 3. MODEL-BASED SEGMENTATION to denote the vector of labels in all voxels except voxel n, and N n the set of voxels that form neighboring pairs with voxel n, we have (Bayes’ rule) p(l n |l \\n ) = p(l) p(l \\n ) = p(l) ∑ ln p(l) = exp(-U (l|θ l )) ∑ ln exp(-U (l|θ l )) = exp ( - β ∑ n 0 ∈Nn δ(l n 0 6= l n ) + log π ln ) ∑ K k=1 exp ( - β ∑ n 0 ∈Nn δ(l n 0 6= k) + log π k ) = π ln · exp ( - β ∑ n 0 ∈Nn δ(l n 0 6= l n ) ) ∑ K k=1 π k · exp ( - β ∑ n 0 ∈Nn δ(l n 0 6= k) ) . (3.19) The second to last step is explained by the fact that all the remaining terms of (3.18) cancel out in the numerator and the denominator. Comparing (3.19) to the standard Gaussian mixture prior, the probability of having label k in voxel n is no longer simply π k , but changes according to the labels assigned to its neighboring voxels. If the majority of neighbors has label k, for instance, the probability of having k in n will be higher than π k . 3.3.2 Inference using the mean-field approximation Combining the more advanced Markov random field prior with the same likeli- hood as before, in which the intensity in each voxel is distributed according to a Gaussian associated with its label ((3.6) and (3.7)), we can in principle eval- uate possible segmentations by comparing their posterior p(l|d, ˆ θ). However, handling this posterior explicitly is difficult in practice because it can no longer be written as a simple product of voxel-wise contributions in the same way as before ((3.12)). There exist fast algorithms, based on so-called graph-cuts, for computing the MAP segmentation ˆ l = arg max l p(l|d, ˆ θ)[6]. However, we are sometimes more interested in calculating expectations, for instance in order to estimate volumes of anatomical structures ((3.3)) or as part of Expectation-Maximization parameter optimizers (which we will cover in Sec. 3.4). Since this involves summing over all possible configurations of l, it is infeasible to do the necessary computations exactly because there are exponentially many configurations of l: a segmentation problem with just K = 2 classes of a standard MR image of size 256 × 256 × 128 voxels yields more than 10 1000000 configurations to sum over 2 ! The solution is to resort to approximation schemes, of which one is a vari- ational method based upon the so-called mean field theory in physics. In this method, we seek a distribution q(l) that we design to be of a more tractable 2 As a comparison, there are approximately 10 80 atoms in the universe. ","3.3. MARKOV RANDOM FIELD PRIORS 39 form than p(l|d, ˆ θ), while still approximating p(l|d, ˆ θ) as well as possible. One possibility is to impose the factorized form of the posterior of the standard Gaussian mixture model ((3.12)) on q(l), i.e., we chose q(l) to be of the form q(l)= N Y n=1 q n (l n ). (3.20) Within this family, our task is to chose the voxel-wise distributions q n (·) in such a way that the resulting joint distribution q(l) approximates p(l|d, ˆ θ) as accurately as possible. For this purpose, we minimize the so-called Kullback- Leibler (KL) divergence KL  q(l) || p(l|d, ˆ θ)  = - X l q(l) log p(l|d, ˆ θ) q(l) , (3.21) which measures how different q(l) is from p(l|d, ˆ θ): it is always positive, and zero only if our approximation q(l) equals the true posterior p(l|d, ˆ θ) exactly (which will typically be unattainable because we restrict the form of q(l) to (3.20)). For a given set of distributions q n 0 (·) in all voxels n 0 6= n, it can be shown [7] that the remaining voxel’s distribution q n (·) that minimizes the KL-divergence is given by q n (l n = k)= N (d n | ˆ μ k , ˆ σ 2 k )γ n (l n = k) ∑ K k 0 =1 N (d n | ˆ μ k 0 , ˆ σ 2 k 0 )γ n (l n = k 0 ) (3.22) with γ n (l n = k)= ˆ π k · exp ( - β ∑ n 0 ∈Nn (1 - q n 0 (l n 0 = k)) ) ∑ K k 0 =1 ˆ π k 0 · exp ( - β ∑ n 0 ∈Nn (1 - q n 0 (l n 0 = k 0 )) ) . (3.23) Comparing this with the voxel-wise posterior of the standard Gaussian mixture model (3.13), it can be seen that the usual class priors π k are replaced with altered priors γ n (k) that take the local neighborhood of the voxels into account: as in (3.19), the number of neighboring voxels “assigned” to a different class is “counted” (in a soft, weighted sense) and alters π k accordingly. Note that (3.22) gives the optimal distribution q n (k) for one voxel at a time, but that this result depends in turn on the result in other voxels. Therefore, a common strategy to minimize the KL-divergence is to cycle through the voxels in turn, updating each voxel’s q n (k) based on the current result in the other voxels, and to continue this process until some convergence criterion is satisfied. With such a minimization strategy, the order in which the voxels are visited as well as the initialization of the q n (l n )’s may affect the local optimum of the KL divergence we arrive at. A segmentation example on the tumor data of fig. 3.3 is shown in fig. 3.4, for different values of the Markov random field parameter β. It can be seen that the more advanced priors add contextual information that improves the segmentation results. ","40 CHAPTER 3. MODEL-BASED SEGMENTATION (a) β =0.25 (b) β =0.55 Figure 3.4: Visualization of the mean-field approximation to the segmentation posterior for the tumor data of Fig. 3.3, for different settings of the Markov random field parameter β. Once the voxel-wise distributions q n (l n ) have been computed, they can be used in the same way as the distributions p(l n |d n , ˆ θ) to approximate expecta- tions, e.g., the expected volume of the structure with label k is approximately given by X l V k (l)q(l)= N X n=1 q n (l n ). (3.24) 3.4 Parameter optimization using the EM algo- rithm So far we have assumed that appropriate values ˆ θ of our model parameters are known in advance. In the tumor segmentation example of the previous section, these parameters were estimated by manually clicking on some representative points in the image, and collecting statistics on the intensity of the selected voxels. In general, however, such a strategy is impractical for such a versatile imaging modality as MRI, where intensities do not directly correspond to any physical properties of the tissue being scanned. By merely tweaking the imaging protocol, upgrading the scanner, or collecting images from different scanner models or manufacturers, the values of ˆ θ become inappropriate and need to be constructed again using manual interaction. This difficulty can be avoided by estimating appropriate values for the model parameters automatically for each individual scan, i.e., each scan receives its own, well-suited set of parameters that are computed without requiring any manual interaction. This can be accomplished by estimating the parameters that maximize the so-called likelihood function p(d|θ), which expresses how probable the observed image d is for different settings of the parameter vector ","3.4. PARAMETER OPTIMIZATION USING THE EM ALGORITHM 41 θ: ˆ θ = arg max θ [p(d|θ)] = arg max θ [log p(d|θ)] . (3.25) The last step is true because the logarithm is a monotonically increasing func- tion of its argument; it is used here because maximizing the log likelihood function instead of the likelihood function directly simplifies the mathematical analysis considerably, and also avoids numerical underflow problems in practi- cal computer implementations. The parameter vector ˆ θ resulting from (3.25) is commonly called the maximum likelihood (ML) parameter estimate. Maximizing the log likelihood function in image segmentation problems is a non-trivial optimization problem for which iterative numerical algorithms are needed. Although a variety of standard optimization methods could potentially be used, for the Gaussian mixture model discussed in Sec. 3.2 a dedicated and highly effective optimizer is available in the form of the so-called expectation- maximization algorithm (EM) 3 . The EM algorithm belongs to a family of op- timization methods that work by repeatedly constructing a lower bound to the objective function, maximizing that lower bound, and repeating the process un- til convergence [8]. This process is illustrated in Fig. 3.5. For a given starting estimate of the model parameters ˜ θ, a function of the model parameters Q(θ| ˜ θ) is constructed that equals the log likelihood function at ˜ θ: Q( ˜ θ| ˜ θ) = log p(d| ˜ θ), (3.26) but that otherwise never exceeds it: Q(θ| ˜ θ) ≤ log p(d|θ), ∀θ. (3.27) The parameter vector maximizing Q(θ| ˜ θ) is then computed and used as the new parameter estimate ˜ θ, after which the whole process is repeated. Critically, because of (3.26) and (3.27), updating the estimate ˜ θ to the parameter vector that maximizes the lower bound automatically guarantees that the log likelihood function increases, by at least the same amount as the lower bound has increased. The consecutive estimates ˜ θ obtained this way are therefore increasingly better estimates of the maximum likelihood parameters – one is guaranteed to never move in the wrong direction in parameter space. This is a highly desirable property for a numerical optimization algorithm. While it is of course always possible to construct a lower bound to an objec- tive function, nothing is gained if optimizing the lower bound is not significantly easier and/or faster to perform than optimizing the objective function directly. However, in the case of the Gaussian mixture model, it turns out it is possible to construct a lower bound for which the parameter vector maximizing it is given directly by analytical expressions. Therefore, the resulting algorithm effectively 3 For more complex models with Markov random field priors, an approximate EM algorithm is obtained by replacing the voxel-wise posteriors with their mean-field approximations. ","42 CHAPTER 3. MODEL-BASED SEGMENTATION (a) (b) (c) (d) (e) (f) Figure 3.5: In the EM algorithm the maximum likelihood parameters ˆ θ are sought by repeatedly constructing a lower bound to the log likelihood function, in such a way that the lower bound touches the log likelihood function exactly at the current parameter estimate ˜ θ (a). Subsequently the parameter estimate ˜ θ is updated to the parameter vector that maximizes the lower bound (b). A new lower bound is then constructed at this new location (c) and maximized again (d), and so forth ((e) and (f)), until convergence. In these plots, the log likelihood function is represented by a full line, and the successive lower bounds with a broken line. ","3.4. PARAMETER OPTIMIZATION USING THE EM ALGORITHM 43 breaks up a difficult maximization problem (of the log likelihood function) into many smaller ones (of the lower bound) that are trivial to solve. Combined with the guarantee of increasingly better estimates of the maximum likelihood parameters as iterations progress, it should be clear why this algorithm is a popular choice for maximum likelihood estimation of Gaussian mixture model parameters. The trick exploited by the EM algorithm to construct its lower bound is based on the property of the logarithm that it is a concave function, i.e., every chord connecting two points on its curve lies on or below that curve (see Fig. 3.6). Mathematically, this means that log [wx 1 + (1 - w)x 2 ] ≥ w log x 1 + (1 - w) log x 2 for any two points x 1 and x 2 and 0 ≤ w ≤ 1. It is trivial to show that this also generalizes to more than two variables, (so-called Jensen’s inequality ): log( K X k=1 w k x k ) ≥ K X k=1 w k log x k (3.28) where w k ≥ 0 and ∑ K k=1 w k = 1, for any set of points {x k }. This can now be used to construct a lower bound to the likelihood function of the Gaussian mix- ture model as follows. Recalling that p(d|θ)= Q N n=1 h ∑ K k=1 N (d n |μ k ,σ 2 k )π k i ((3.10) and (3.11)), we have that log p(d|θ) = log  N Y n=1 \" K X k=1 N (d n |μ k ,σ 2 k )π k #! (3.29) = N X n=1 log \" K X k=1 N (d n |μ k ,σ 2 k )π k # (3.30) = N X n=1 log \" K X k=1  N (d n |μ k ,σ 2 k )π k ω n,k  ω n,k # (3.31) ≥ N X n=1 \" K X k=1 ω n,k log  N (d n |μ k ,σ 2 k )π k ω n,k  # | {z } Q(θ| ˜ θ) , (3.32) for any set of weights {ω n,k } that satisfy ω n,k ≥ 0 and ∑ K k=1 ω n,k = 1 (the last step relies on (3.28)). We now have a lower bound function Q(θ| ˜ θ) that satisfies (3.27), but not (3.26), so we are not done yet. Instead of randomly assigning any valid K weights ω n,k to each voxel n (one weight for each label k), we can satisfy (3.26) by choosing the weights so that ω n,k = N (d n | ˜ μ k , ˜ σ 2 k )˜ π k ∑ K k 0 =1 N (d n | ˜ μ k 0 , ˜ σ 2 k 0 )˜ π k 0 . (3.33) ","44 CHAPTER 3. MODEL-BASED SEGMENTATION Figure 3.6: Because the logarithm is a concave function, the cord between any two points on its curve lies on or below the curve. An example cord is shown in blue. By filling these weights into the definition of our lower bound (3.32), it is easy to check that (3.26) is indeed fulfilled with this choice. Setting the new model parameter estimate ˜ θ to the parameter vector that maximizes the lower bound requires finding the location where ∂Q(θ| ˜ θ) ∂ θ =0. Reformulating the lower bound as Q(θ| ˜ θ) = - 1 2 K X k=1 \" 1 σ 2 k N X n=1 ω n,k (d n - μ k ) 2 +  N X n=1 ω n,k ! log σ 2 k # + K X k=1 \" N X n=1 ω n,k ! log π k # - N X n=1 K X k=1 ω n,k log ω n,k - N 2 log (2π) , (3.34) it is straightforward to show that the parameter updates are given by ˜ μ k ← ∑ N n=1 ω n,k d n ∑ N n=1 ω n,k ˜ σ 2 k ← ∑ N n=1 ω n,k (d n - ˜ μ k ) 2 ∑ N n=1 ω n,k (3.35) ˜ π k ← ∑ N n=1 ω n,k N . ","3.5. MODELING MR BIAS FIELDS 45 (a) Initialization (b) After one iteration (c) After 11 iterations (d) After 30 iterations Figure 3.7: Iterative improvement of the Gaussian mixture model parameters for the MR image of Fig. 3.1a, using the EM algorithm. It is worth spending some time thinking about these equations. The EM algorithm searches for the maximum likelihood parameters of the Gaussian mix- ture model merely be repeatedly applying the update rules of (3.35), where the weights ω n,k are defined in (3.33). These weights depend themselves on the current estimate of the model parameters, which explains why the algorithm involves iterating. More importantly, by comparing (3.33) to (3.13), we see that these weights represent nothing but the posterior probability of the segmen- tation, given the current model parameter estimate! Thus, the EM algorithm repeatedly computes the type of probabilistic segmentation shown in Fig. 3.2 based on its current parameter estimate, and then updates the parameter es- timate accordingly. The update rules of (3.35) are remarkably intuitive: the mean and variance of the Gaussian distribution associated with the kth label are simply set to the weighted mean and variance of the intensities of those voxels currently attributed to that label; similarly the prior for each class is set to the fraction of voxels currently attributed to that class. Fig. 3.7 shows a few iterations of the EM algorithm searching for the maxi- mum likelihood parameters of the brain MR data shown in Fig. 3.1a. 3.5 Modeling MR bias fields Although the Gaussian mixture model is a very useful tool for image segmen- tation, and comes with a an dedicated parameter estimation algorithm, it can often not be applied directly to MR images. This is because MR suffers from an imaging artifact that makes some image areas darker and other areas brighter than they should be. This spatially smooth variation of intensities is often re- ferred to as MR “intensity inhomogeneity” or “bias field”, and is caused by imaging equipment limitations and electrodynamic interactions with the object being scanned. Interestingly, the bias field artifact is dependent on the anatomy ","46 CHAPTER 3. MODEL-BASED SEGMENTATION (a) 1.5 Tesla (b) 3 Tesla (c) 7 Tesla Figure 3.8: The MR bias field artifact is often more pronounced in scanners operating at higher magnetic field strengths. being imaged and therefore unique to each scan session, and is much more pro- nounced in the newest generation of scanners (see Fig. 3.8). Since the Gaussian mixture model does not account for smoothly varying overall intensity levels within one and the same anatomical structure, it is very susceptible to segmentation errors when applied to typical MR data. However, this problem can be avoided by explicitly taking a model for the bias field artifact into account in the imaging component of the generative model. In particular, we can model the artifact as a linear combination of M spatially smooth basis functions M-1 X m=0 c m φ n,m , (3.36) where φ n,m is shorthand for φ m (x n ), the value of the mth basis function evalu- ated at voxel n, which has spatial location x n . Suitable basis functions can be 2D DCT basis functions, 2D B-spline basis functions (such as those shown in Fig. 1.5), or something similar. We can then extend the imaging model of the Gaussian mixture model by still assigning each voxel an intensity drawn from a Gaussian distribution associated with its label, but further adding 4 the bias model to the resulting intensity image to obtain the final, bias field corrupted image d. With this model, we have p(d|l, θ d )= N Y n=1 N  d n   μ ln + M X m-1 c m φ n,m ,σ 2 ln  (3.37) with parameters θ d =(μ 1 ,...,μ K ,σ 2 1 ,...,σ 2 K ,c 0 ,...,c M-1 ) T , which consist not only of the parameters associated with the Gaussian distributions, but ad- ditionally also the M coefficients of the bias field basis functions, c m . 4 Because of the physics of MR, the bias field is better modeled as a multiplicative rather than an additive artifact. This can be taken into account by working with logarithmically transformed intensities in the models, instead of using directly the original MR intensities. ","3.5. MODELING MR BIAS FIELDS 47 As was the case with the Gaussian mixture model, model parameter estima- tion can be performed conveniently by iteratively constructing a lower bound to the log likelihood function, and working with that lower bound instead. For constructing the lower bound in this case, we follow the exact same procedure as in the previous section to obtain Q(θ| ˜ θ) = - 1 2 K X k=1   1 σ 2 k N X n=1 ω n,k  d n - μ k - M X m-1 c m φ n,m ! 2 +  N X n=1 ω n,k ! log σ 2 k   + K X k=1 \" N X n=1 ω n,k ! log π k # - N X n=1 K X k=1 ω n,k log ω n,k - N 2 log (2π) , (3.38) where now the weights satisfying (3.26) are given by ω n,k = N  d n    ˜ μ k + ∑ M m-1 ˜ c m φ n,m , ˜ σ 2 k  ˜ π k ∑ k 0 N  d n    ˜ μ k 0 + ∑ M m-1 ˜ c m φ n,m , ˜ σ 2 k 0  ˜ π k 0 . (3.39) Maximizing this lower bound is more complicated than in the Gaussian mixture model, however, because setting the derivative with respect to the parameter vector θ to zero no longer yields analytical expressions for the parameter update rules. If we keep the bias field parameters fixed at their current values ˜ c m , and only maximize the lower bound with respect to the Gaussian mixture model parameters, we easily obtain ˜ μ k ← ∑ N n=1 ω n,k  d n - ∑ M m-1 ˜ c m φ n,m  ∑ N n=1 ω n,k ˜ σ 2 k ← ∑ N n=1 ω n,k  d n - ∑ M m-1 ˜ c m φ n,m - ˜ μ k  2 ∑ N n=1 ω n,k (3.40) ˜ π k ← ∑ N n=1 ω n,k N . Similarly, keeping the Gaussian mixture model parameters fixed at their current values, it is easy to see that the bias field parameters maximizing the lower bound are given in analytical form as well: the lower bound is a quadratic function of the coefficients c m , and finding their optimal values is therefore merely a linear basis function regression problem (see Sec. 1.1). In particular, the solution is given by ˜ c ← (Φ T PΦ) -1 Φ T Pr (3.41) ","48 CHAPTER 3. MODEL-BASED SEGMENTATION where Φ is given by (1.2) and p n,k = ω n,k ˜ σ 2 k , p n = K X k=1 p n,k , P = diag(p 1 ,...,p N ), ˜ d n = ∑ K k=1 p n,k ˜ μ k ∑ K k=1 p n,k , r =    d 1 - ˜ d 1 . . . d N - ˜ d N    . Since (3.40) and (3.41) depend on one another, one could in principle try to maximize the lower bound by cycling through these two equations, one at a time, until some convergence criterion is met. However, the desirable property of the EM algorithm to never decrease the value of the likelihood function with each new iteration still holds even when the lower bound is not maximized but merely improved. Therefore, a more efficient strategy is to construct the lower bound by computing the weights ω n,k ((3.39)) and then update the Gaussian mixture model parameters ((3.40)) and subsequently the bias field parameters ((3.41)) only once to merely improve it. After that a new lower bound is con- structed by recomputing the weights, which is again improved by updating the model parameters, etc, until convergence. Such an optimization strategy of only partially optimizing the EM lower bound is known as so-called generalized expectation-maximization. The interpretation of the update equations is again very intuitive. As was the case in the Gaussian mixture model, the weights ω n,k represent again a statistical classification of the image voxels as in Fig. 3.2, and the mixture model parameters are again updated accordingly. The only difference now is that instead of the original MR intensities, d n , the bias field corrected intensities, d n - ∑ M m-1 ˜ c m φ n,m , i.e., the intensities after the estimated bias field has been subtracted, are used. Regarding the bias field update, the algorithm tries to make a reconstruction ( ˜ d 1 ,..., ˜ d N ) T of what the image should look without the bias field artifact (shown in Fig. 3.9b), subtracts that from the MR scan to obtain a (noisy) estimate of the bias field (image r, shown in Fig. 3.9c), and smoothes the result to obtain an estimate of the bias field (shown in Fig. 3.9e). For the smoothing, each voxel has a weight p n (shown in Fig. 3.9d) that depends on the variance of the class it is attributed to, reflecting the confidence in the local value of r (classes with tighter variances are more trustable than classes with very large variances). By extending the Gaussian mixture model with an explicit model for the bias field artifact this way, it is possible to obtain high-quality segmentations of MR scans without errors caused by intensity inhomogeneities, as shown in Fig. 3.10. ","3.5. MODELING MR BIAS FIELDS 49 (a) MR scan (b) Reconstruction (c) Difference between the MR scan and the recon- struction (d) Weight image (e) Estimated bias field Figure 3.9: Illustration of the bias field estimation within a generalized expectation-maximization algorithm. ","50 CHAPTER 3. MODEL-BASED SEGMENTATION (a) (b) (c) (d) Figure 3.10: Explicitly modeling and estimating the bias field artifact in MR scans often improves segmentation results considerably. Shown are a few sagittal slices from a brain MR scan (a); the posterior probability for white matter using the standard Gaussian mixture model (b); the same when a bias field model is explicitly taken into account (c); and the automatically estimated bias field model (d). Note the marked improvement in segmentation accuracy in the upper parts of the brain. ","Chapter 4 Neural Networks The methods for registration and segmentation that we have seen so far are all based on models that somehow encode prior knowledge of the problem at hand. For instance, mutual information-based registration exploits the fact that one image is predictive of another image only when the two are well aligned; while the Gaussian mixture model for segmentation encodes the knowledge that voxels with the same label typically have similar intensities. A completely different approach to solving a problem is not to try to un- derstand it, but rather to simply emulate successful example solutions. As opposed to the generative models of Chapter 3, this approach to model-free “machine learning” is called discriminative learning. In this chapter we will review a specific class of discriminative methods based on artificial neural net- works. Although such networks can also be used for other purposes, such as image registration and computer aided diagnosis, they are especially successful in the area of image segmentation, which we will focus on in this chapter. In general, it should be noted that sidestepping the difficulty of building models is both the main strength and the most important weakness of neural networks. As long as sufficient example solutions are available, they are very easy to deploy without requiring any domain-specific knowledge, and can be orders of magnitude faster than model-based methods. On the other hand, however, example solutions are often in short supply in medical image analysis, as manually annotating a large set of images can be excruciatingly time con- suming. Especially in versatile imaging modalities such as MRI, this problem is exacerbated by the differences in scanning hardware, software and protocols that exist even within the same hospital, which can make carefully curated example solutions useless overnight. 4.1 Logistic regression In Chapter 3 we saw how the Gaussian mixture model can be used to classify each image voxel into one of K different classes (tissue types) based on its inten- 51 ","52 CHAPTER 4. NEURAL NETWORKS sity alone. Specifically, we modeled the prior probability of a voxel belonging to class k as π k , and the intensity distribution associated with class k as a Gaus- sian with mean μ k and variance σ 2 k . Using Bayes’ rule, we then obtained the probability of a voxel having label l as (cf. ((3.13))) p(l = k|d, θ)= N (d|μ k ,σ 2 k )π k ∑ k 0 N (d|μ k 0 ,σ 2 k 0 )π k 0 , (4.1) where d is the voxel’s intensity and θ are the parameters of the model (collecting π k , μ k and σ 2 k for all classes). Rather than using such models, another way of obtaining a voxel-wise clas- sifier p(l|d, θ) is by simply mimicking the behavior seen in (“learning from”) examples. For reasons that will soon become clear, for the remainder of this chapter we will switch to the notation used for regression in Chapter 1, indi- cating a training set of observed example inputs and outputs as {x n ,t n } N n=1 , where x n is the D-dimensional vector of observed inputs for the nth case, and t n the corresponding output. For voxel-wise classification, each input is simply an intensity x n = d n , i.e., the input dimensionality is D = 1, but we will soon see cases where x n is higher-dimensional. Unlike in the regression case, where each output t n was a continuous value, for classification the outputs can only take K discrete values, indicating which class each example belongs to. To simplify notation, here we only consider scenarios with K = 2 classes 1 . The outputs can then be denoted as taking binary values t n ∈{0, 1}, where values 1 and 0 correspond to the class assignments l n = 1 and l n = 2, respectively. To model p(y|x, θ) – which is equivalent to modeling p(l|d, θ) in our voxel- wise classification example – we start from a parametric curve of the form f (x)= σ  M-1 X m=0 w m φ m (x) ! , (4.2) which is similar to the linear basis function model for regression we saw in Sec. 1.1: A linear combination of M nonlinear basis functions φ m (x) with tunable coefficients w m , which we here collect in the parameter vector θ = (w 0 ,...,w M-1 ) T . Unlike in the regression case, however, where the quantity a = M-1 X m=0 w m φ m (x) (4.3) was used directly, here it is subject to a “squashing” function (illustrated in Fig. 4.1) σ(a)= 1 1 + exp(-a) (4.4) that maps the result into the interval [0, 1]. Because of this mapping, f (x) can be interpreted as a probability: p(t =1|x, θ)= f (x). Since p(t =0|x, θ)= 1 The generalization to several classes is rather straightforward (cf. [9], chapter 4). ","4.2. TRAINING WITH STOCHASTIC GRADIENT DESCENT 53 Figure 4.1: The logistic function σ(a) defined in ((4.4)) maps the domain [-∞, ∞] into [0, 1]. 1 - p(t =1|x, θ)=1 - f (x), we can then write (Bernoulli distribution): p(t|x, θ)= f (x) t [1 - f (x)] 1-t . (4.5) Because the mapping function σ(a) in ((4.4)) is called the “logistic function”, the model of ((4.2)) is known as “logistic regression”. Given our training set of N input observations X = {x 1 ,..., x N } with corre- sponding outputs t = {t 1 ,...,y N }, appropriate values for the model parameters ˆ θ can be found by numerically maximizing the likelihood function p(t|X, θ)= N Y n=1 p(t n |x n , θ) (4.6) with respect to θ. Once fitted this way, the model can subsequently be used to classify new voxels simply by evaluating p(l =1|d, ˆ θ)= p(t =1|x, ˆ θ)= σ  M-1 X m=0 ˆ w m φ m (x) ! , and assigning a voxel to class 1 if p(l =1|d, ˆ θ) > 0.5 and to class 2 otherwise. Fig. 4.2 illustrates this procedure on an MRI scan of the kidneys, using the five 1D cosines of Fig. 1.1a as basis functions φ m (x) and N = 50 training points. 4.2 Training with stochastic gradient descent So far we have not specified how to numerically optimize ((4.6)) during train- ing. For the specific case of logistic regression, there exists a dedicated algo- rithm called iterative reweighted least squares (IRLS) that finds ˆ θ by iteratively ","54 CHAPTER 4. NEURAL NETWORKS (a) (b) (c) (d) (e) Figure 4.2: Example of a logistic regression classifier trained to discern pixels in the inner organs from pixels in other areas. (a) Image to be segmented, along with training data consisting of 25 manually selected points inside (l = 1, blue circles) and 25 points outside (l = 2, red crosses) the area of interest. (b) The output p(l =1|d, ˆ θ) of the trained classifier, evaluated in each pixel. (c) Final segmentation produced by the trained classifier, overlaid on top of the input image. (d) The quantity a of ((4.3)) for various intensity levels d. (e) The classifier resulting from subjecting a to the squashing function σ(a) of ((4.4)). For reference, the intensity levels and labels of the 50 training points are also shown. ","4.3. FEED-FORWARD NETWORK FUNCTIONS 55 mapping the problem to a regression problem, which is then solved using tools similar to those described in Chapter 1. However, for the more general case of feed-forward neural networks, which we will soon discuss, IRLS cannot be used and optimization is typically performed using variants of stochastic gradient descent, described next. Maximizing the likelihood function p(y|X, θ) is equivalent to minimizing its negative logarithm. Using ((4.5)) and ((4.6)), training therefore consists of minimizing the energy function E N (θ)= - log p(t|X, θ)= - N X n=1 {t n log f (x n ) + (1 - t n ) log [1 - f (x n )]} , (4.7) which is known in the machine learning community as cross-entropy. Start- ing from some (e.g., random) initial estimate θ (0) , standard gradient descent proceeds by iteratively stepping in the direction of steepest descent: θ (τ +1) = θ (τ ) - ν ∇E N (θ (τ ) ), where τ denotes the iteration number, ∇E N (θ)= ∂E N ∂θ is the gradient of the energy function, and ν is a step size that needs to be provided by the user. In practice, significant computational speed-ups can be achieved by noticing that E N (θ) and therefore ∇E N (θ) involves summing over the contribution of all N training points. Since typically there is considerable redundancy in the training data set, i.e., many training points will contribute similarly, in stochastic gradient descent the true gradient is approximated by ∇E N (θ) ’ N N 0 ∇E N 0 (θ), where ∇E N 0 (θ) is the gradient computed from only N 0  N randomly sampled training points. Optimizing then proceeds by θ (τ +1) = θ (τ ) - ν 0 ∇E N 0 (θ (τ ) ) (4.8) with step size ν 0 = Nν/N 0 , after which a new subset of size N 0 is sampled for the next iteration. 4.3 Feed-forward network functions Although we have so far only considered a problem where the dimensionality of x is D = 1, classifiers can also be used in cases where D  1. As an example, considering again the same MRI slice of the kidneys as before, but this time we are given a full manual delineation of the border just outside of the body as training data (shown in blue in Fig. 4.3b). Our task is now to automatically segment the same border in other MRI slices and/or patients (an example is shown in Fig. 4.3c). Since the border cannot be discerned based on intensity alone, a simple voxel-wise classifier will no longer suffice. Instead, we can take ","56 CHAPTER 4. NEURAL NETWORKS (a) (b) (c) Figure 4.3: Training image (a) and corresponding segmentation (b) used for training the network shown in Fig. 4.4, as well as the type of image (c) that the network should be able to segment after training. the local context into account, by having as input x not just the intensity of the pixel we aim to classify, but also that of its immediate 8 neighbors in the 2D pixel grid. The input to the classifier is therefore not a scalar intensity, but rather an entire 3 × 3 image patch, so that the dimensionality is D = 9. In order to use logistic regression as before, we now face the difficulty of choosing appropriate basis functions φ m (x) in our 9-dimensional input space. For low-dimensional cases (e.g., D = 2 or D = 3) we could follow the same procedure as in Sec 1.3 and use separable basis functions, which are simply the product of 1D basis functions as in (1.11). However, this construct quickly becomes impractical in higher dimensions since the number of basis functions in- creases exponentially with the dimensionality (“curse of dimensionality”): with our five 1D cosines we would obtain 5 9 basis functions when D = 9, which is almost 2 million basis functions! Rather than using a set of fixed basis functions, one solution is to choose basis functions that are adaptive, by having their form depend on extra parameters that are also optimized during training. A typical choice of such adaptive basis functions is as follows: φ m (x)= ( 1 if m =0, σ  ∑ D d=1 β m,d x d + β m,0  otherwise, (4.9) where σ(·) is the logistic function defined in ((4.4)), and the weights β m,d are extra parameters that together with the coefficients w m form the parameters θ of the model. Fig. 4.4 has a graphical representation of the resulting model for the case D = 9 and M = 4. Loosely based on a (now completely outdated) notion of similarity with how information is processed in the brain, it presents a simple feed-forward neural network in which information moves from the left of the figure to the right: all the components x d (called input units in the neural network literature) of x are linearly combined and then non-linearly transformed through the logistic function to evaluate the basis functions φ m (x)’s (called hidden units ). All the resulting φ m (x)’s are then themselves linearly combined ","4.3. FEED-FORWARD NETWORK FUNCTIONS 57 Figure 4.4: Graphical representation of a feed-forward neural network with a 9-dimensional input, a single hidden layer with three hidden units, and a single output unit. Since the parameters w 0 and {β m,0 } effectively add mere constants during the computations, they are shown to be connected to additional units with values clamped to 1 (indicated by filled circles). The information flows from the left to the right through the network. and non-linearly transformed in the same way to obtain f (x) (called the output unit ). It is easy to see that this type of model can be readily extended by inserting more layers of hidden units, each taking the previous hidden layer as input, resulting in “deep” networks for models with many such layers. Because of their specific structure, the gradient ∇E N (θ) that is needed for training these networks can be computed very efficiently, even for very large networks with many parameters, using an algorithm known as backpropagation [10]). Fig. 4.5 shows the result of applying our 3 × 3 patch-based network on a new image, after training it on the image-segmentation pair of Fig. 4.3b using stochastic gradient descent. The network output f (x) for each pixel is shown in Fig. 4.5b, and the basis function evaluations {φ m (x)} M-1 m=1 (called feature maps ) and their coefficients {w m } M-1 m=1 are shown in Fig. 4.5d. Since we are working with 3 × 3 images patches, the set of weights {β m,d ,d =1,..., 9} for each feature m =1,..., 3 has a spatial structure and can be visualized as a 3 × 3 image itself, shown in Fig. 4.5e. Since computing the term ∑ D d=1 β m,d x d in ((4.9)) for each pixel in the input image can be implemented as a convolution of the image with a 3 × 3 spatial filter, our network is a simple instance of what is called a convolutional neural network [11]. In practical applications, such networks typically have many more hidden layers than just the one used here, effectively using the feature maps shown in Fig. 4.5d as input to the next convolutional network layer etc, yielding increasingly higher-level features that can be used to solve increasingly hard segmentation tasks. The price to be paid for this increased ability is that the network then has many more parameters to ","58 CHAPTER 4. NEURAL NETWORKS be estimated, which necessitates access to much larger amounts of (often very hard to get) annotated training data. ","4.3. FEED-FORWARD NETWORK FUNCTIONS 59 (a) (b) (c) (d) (e) Figure 4.5: Segmentation of a new image using the patch-based network of Fig. 4.4, after training it on the data of Fig. 4.3b:(a) image to be segmented; (b) posterior probability generated by the network; (c) final segmentation generated by the network; (d) the feature maps used by the network to generate the segmentation, along with the corresponding coefficients; and (e) the 3 × 3 weight patterns used to generate the feature maps. ","60 CHAPTER 4. NEURAL NETWORKS ","Chapter 5 Atlases In this chapter we discuss the concept of so-called atlases in biomedical im- age analysis. An atlas is broadly defined as an image that has somehow been augmented with additional information beyond the voxel intensities alone. The exact form of this additional information depends on the specific application the atlas is used for, but typical examples include detailed manual segmenta- tions of all the structures that are visible in the image, or a reference coordinate system that allows to compare anatomical or functional characteristics across different individuals. Atlases are often used as a teaching tool, helping medical students understand the complicated three-dimensional shape, configuration, and relations of different anatomical structures, or as an anatomical reference in surgical planning. They also frequently serve as a means of incorporating detailed anatomical knowledge into automated segmentation algorithms, or as a substrate to report novel findings in the scientific literature. In this chapter we only concentrate on two types of atlases, namely so- called reference templates and atlases for automated segmentation. Although the examples in this chapter are all from brain MRI, the same concepts apply equally well to other imaging modalities such as CT and PET, and different anatomical structures, including the heart and lungs. 5.1 Reference templates A reference template T is an image that serves as a reference coordinate system. By registering other images to this template and transferring relevant functional or anatomical information accordingly, findings can be compared across individ- uals, with subject-specific anatomical differences removed. Such a spatial nor- malization of information has many applications, including understanding the normal variation in anatomy between different individuals, comparing anatomy or function between patient populations to understand disease effects, or com- municating scientific findings to researchers working in different laboratories. Mathematically, once a template image T has been selected, it can be used 61 ","62 CHAPTER 5. ATLASES as a reference coordinate system by mapping any image under study, denoted by I , into this standard coordinate system. Using the notation from Chapter 2, a geometrical transformation y(x, w) is computed that maps the coordinates x n of the template’s voxels to coordinates y(x n , w) in the image. Resampled intensity values I (y(x n , w)) can then be extracted, effectively deforming the image into the reference coordinate system. 5.1.1 Intensity averaging In its simplest form, we can just pick some representative scan of a randomly chosen subject to serve as the template T . However, it is often desirable to use a template that is more representative of a whole population, because the anatomy of a single individual cannot faithfully represent the complex structural variabil- ity between different people. This can be accomplished by taking a collection of images {I 1 ,..., I Q } acquired from Q different individuals, performing Q - 1 registrations between the scans of individuals 2 ...Q and the scan of the first individual, and averaging the resulting images. Specifically, let the parameter vector of the geometrical transformation mapping individual 1 to individual q be denoted by w q . For each voxel n in the scan of individual 1, with coordi- nates x n and intensity I 1 (x n ), we can then obtain the corresponding intensity I q (y(x n , w q )) in each subject q =2 ...Q. The template T is then computed by assigning as intensity of voxel n the average intensity over all Q available images: T (x n ) ← I 1 (x n )+ ∑ Q q=2 I q (y(x n , w q )) Q . The level of anatomical detail in such average templates depends on the degrees of freedom in the geometrical transformation y(x, w): models with many degrees of freedom will be able to match corresponding anatomical locations across all Q individuals better than simpler models, yielding “sharper” templates. Example reference templates illustrating this effect are shown in Fig. 5.1. 5.1.2 Group-wise registration Although the procedure outlined above effectively averages over the intensity levels across Q different scans, the geometrical shape of the template is still entirely defined by the shape of the first individual, since the scan of that first individual was used to establish the coordinate system to which all other scans were deformed. This can be problematic in studies comparing two different groups of subjects, if the shape of the first individual is somehow closer to the average shape of one of the groups: the registration process will then be more difficult to perform for the other group, resulting in an uneven distribution of registration errors and ultimately false interpretations about the inferred differences between the groups. In order to avoid this situation, it is better to compute a template that is average both in terms of intensities and anatomical shape. This can be accom- plished by considering the following generative model for the Q available scans. ","5.1. REFERENCE TEMPLATES 63 (a) (b) Figure 5.1: Reference template obtained by intensity-averaging the brain MR scans of 20 different individuals after affine (a) and deformable (a) registration. First, a template image T is assumed to be drawn from some prior distribution p(T ). Since we typically have no reason to prefer certain templates over oth- ers, we will use a uniform prior, i.e., p(T ) ∝ 1. Subsequently, each scan I q is assumed to be obtained independently from this template by (1) drawing a sam- ple w q from some distribution p(w) ∝ exp (-S (w)) governing the deformation model parameters, where S (w) is a regularizer penalizing unlikely deformations; (2) deforming the template T accordingly; and (3) adding random, zero-mean Gaussian noise with variance σ 2 to each voxel independently. With this model, the template T and the deformation parameters {w q } can be estimated from the Q available scans by maximizing their joint posterior distribution p(T , w 1 ,..., w Q |I 1 ,... I Q ) ∝ p(I 1 ,... I Q |T , w 1 ,..., w Q )p(T , w 1 ,..., w Q ) = p(T ) Q Y q=1 p(I q |T , w q )p(w q ) ∝ Q Y q=1 \" N Y n=1 exp  - (I q (y(x n , w q )) -T (x n )) 2 2σ 2 !# exp (-S (w q )) . (5.1) This is equivalent to minimizing - log [p(T , w 1 ,..., w Q |I 1 ,... I Q )], which can be re-written as: { ˆ T , ˆ w 1 ,..., ˆ w Q } = arg min {T ,w1,...,w Q } Q X q=1 \" 1 2σ 2 N X n=1 (I q (y(x n , w q )) -T (x n )) 2 + S (w q ) # . (5.2) Starting from some initial deformation parameters ˜ w q , typically chosen to cor- ","64 CHAPTER 5. ATLASES respond to no deformation at all, the optimization of (5.2) can be performed by updating the estimate of the template ˜ T while keeping the deformation param- eters fixed to their current values, and subsequently updating the deformation parameters while keeping the template fixed, each in turn, until convergence. The update for the template that minimizes the objective function of (5.2) for a given set of deformation parameters is given by ˜ T (x n ) ← ∑ Q q=1 I q (y(x n , ˜ w q )) Q (5.3) for each template voxel n, and the corresponding update for each of the Q deformation parameter vectors is given by ˜ w q ← arg min wq \" 1 2σ 2 N X n=1  I q (y(x n , w q )) - ˜ T (x n )  2 + S (w q ) # , (5.4) which can be solved using the optimization procedure discussed in Sec. 2.4.1. In summary, the algorithm computes an initial, fuzzy average template ((5.3)) to which all Q scans are subsequently registered ((5.4)), after which the template is re-computed etc, until convergence. The template resulting from this procedure will show increasingly more anatomical detail as the iterations progress and the deformations to it become more precise. Upon convergence, a template is obtained that it is not biased to any of the Q available scans in particular, but rather represents the average, both in shape and intensity, among all the scans simultaneously. 5.2 Atlases for segmentation Another class of atlases is used to infuse prior anatomical knowledge into com- putational image segmentation algorithms. In this setting, one starts not only from Q medical images {I 1 ,..., I Q }, but also from a corresponding number of detailed manual segmentations {L 1 ,..., L Q } of those images. To keep notation consistent with the one used in Chapter 3, we will assume that each voxel in those segmentations is assigned a unique label k ∈{1,...,K} that represents the anatomical structure the voxel belongs to. For each individual q, we thus have an intensity I q (x) and a corresponding label L q (x) in each location x. Atlases for segmentation purposes come in two distinct forms. So-called probabilistic atlases contain pre-computed statistics about the Q label images {L q }, whereas atlases used for so-called label propagation use the original label images directly instead. Below we will discuss the basic principles of both types of segmentation atlases. 5.2.1 Probabilistic atlases In Chapter 3 we introduced two priors that are often used in model-based seg- mentation. The first, stated in (3.5), simply assumes that label k occurs with ","5.2. ATLASES FOR SEGMENTATION 65 (a) (b) (c) Figure 5.2: Samples from three different priors often used in model-based seg- mentation: the prior used in the standard Gaussian mixture model (a); a Markov random field prior (b); and a probabilistic atlas prior (c). probability π k in any given voxel, without any further constraints on the spa- tial organization of the labels. The second, stated in (3.16) and (3.18), is a Markov random field model that additionally encourages the different labels to occur in spatial clusters, rather than being scattered randomly throughout the image area. Although these priors are computationally convenient to work with, they do not encode any information about the shape, organization, and spatial relationships of real anatomical structures, as demonstrated in Fig. 5.2a and 5.2b). Here we consider a third class of priors for model-based segmentation, namely probabilistic atlases, that do encode such prior knowledge of anatomy while still being computationally attractive. A sample from this type of anatomical prior is shown in Fig. 5.2c. The prior is constructed as follows. First, an intensity template T is computed by co-registering all the Q intensity images {I q } to a common reference and averaging the intensities, using one of the techniques discussed in Sec. 5.1. Subsequently, the qth warp y(x, w q ) mapping the intensity image I q to the template is used to warp the corresponding segmentation L q into the template space as well. Finally, at every voxel n with location x n in the template, one counts the frequency with which each label k occurred at that location across the Q warped label images: π n,k = ∑ Q q=1 δ (L q (y(x n , w q )) = k) Q , (5.5) where δ(k = l) equals one if k = l and zero otherwise. The resulting π n,k repre- sents roughly the prior probability that label k occurs in voxel n in the template space: it varies spatially and always satisfies ∑ K k=1 π n,k = 1 in all voxels. Ex- amples of such prior probability maps derived from manual segmentation of the brain’s white matter, gray matter and CSF are shown in Fig. 5.3. Once the template T and the prior probabilities π n,k have been computed, ","66 CHAPTER 5. ATLASES (a) (b) Figure 5.3: Probabilistic atlas of the main brain tissue types using affine reg- istration (a) and deformable registration (b). The atlases represent spatially varying prior probability maps of white matter, gray matter, CSF, and every- thing else. Bright and dark intensities correspond to high and low probabilities, respectively. ","5.2. ATLASES FOR SEGMENTATION 67 they can be used to segment a new image I as follows. First, the geometric transformation between T and I is computed, which is then applied to the prior probability maps, yielding interpolated prior probabilities π n,k for every class k in every voxel n in I . These prior probabilities π n,k are then used in place of the generic π k in every equation of the model-based segmentation models of Chapter 3, yielding voxel classifications that no longer depends solely on the voxels’ local intensity alone, but also on their spatial location. Furthermore, the priors π n,k unambiguously associate segmentation classes to pre-defined anatom- ical structures, and can be used to automatically initialize the iterative update equations of the EM optimizers, even in multi-channel data (vector-valued voxel intensities) where initialization is otherwise difficult. Finally, the spatial priors are also typically used to discard voxels that are of no interest, such as muscle, skin, or fat in brain MR scans. As a result, the use of the spatial priors π n,k con- tributes greatly to the overall robustness and practical value of the model-based segmentation models discussed in Chapter 3. 5.2.2 Label propagation In so-called label propagation, an image I is segmented by computing a geo- metrical transformation y(x n , w q ) that maps each voxel location x n in I into a corresponding location in the qth pre-segmented image I q . Each voxel’s label is then simply propagated from the manual segmentation L q associated with I q , i.e., the label assigned to voxel n in I is given by L(x n ) ← L q (y(x n , w q )). This process is illustrated in Fig. 5.4. Of course, the same procedure can be followed for each of the Q pairs {I q , L q }, yielding Q possible segmentations of the same image I . In practice, it is found that combining all Q segmentations in some way yields a consensus segmentation that is of a much higher quality than any of the Q original seg- mentations considered individually. Although there are many possible ways to combine Q segmentations of the same image, a very simple but still effective method is so-called majority voting, where each voxel is assigned to the label that occurred most frequently among all Q individual segmentations: L(x n ) ← arg max k \" Q X q=1 δ (L q (y(x n , w q )) = k) # . ","68 CHAPTER 5. ATLASES (a) (b) (c) (d) (e) Figure 5.4: In label propagation an image (a) is automatically segmented by taking an image from another subject (b) that has been manually segmented (c), computing the deformation that warps the pre-segmented image to the to-be-segmented image (d), and applying the same deformation to the manual segmentation (e). The result (e) is an automatic segmentation of the to-be- segmented image (a). ","Chapter 6 Validation An important aspect of developing medical image analysis algorithms is demon- strating that the resulting algorithms actually work. This is called validation, and it often entails quantitatively evaluating how automatically generated re- sults compare to those obtained by a trained human expert. Although validation is vital in both registration and segmentation, in this chapter we will consider only the most important validation strategies for seg- mentation applications. Specifically, we will discuss two different validation scenarios: in the first scenario automatic segmentations are compared directly to manual segmentations that are considered to perfectly reflect the underlying anatomy, whereas in the second scenario we first need to estimate the underlying anatomy from several imperfect manual segmentations. 6.1 Validation against a known ground truth Consider a scenario where someone has developed a new automated segmen- tation algorithm, and wants to demonstrate its performance. For a number of different images that are representative of what is encountered in the target application area, (s)he has access to manual segmentations that have been care- fully performed by a trained human expert. The task now is to evaluate how well the automatically generated segmentations compare to the manual ones. For the remainder of this section, we will concentrate on how to compare a single manual segmentation with a corresponding automated one, keeping in mind that in practice one would analyze dozens of different cases and summarize the results, for instance by averaging the performance over the individual cases. 6.1.1 Confusion matrix, sensitivity, and specificity To establish notation, let t =(t 1 ,...,t N ) T denote a manual segmentation of an image with N voxels, where t n ∈{0, 1} denotes the label assigned to voxel with index n. We will refer to t as the ground truth segmentation. By convention, 69 ","70 CHAPTER 6. VALIDATION t n = 1 if the voxel belongs to the structure of interest (so-called “foreground”), and t n = 0 otherwise (“background”). Similarly, the automated algorithm to be evaluated generates a segmentation s =(s 1 ,...,s N ) T ,s n ∈{0, 1} that is (hopefully) similar to t, but not exactly the same. The number of true positives (TP) is defined as the number of voxels that are assigned to the foreground in both s and t, i.e., TP = N X n=1 s n t n . (6.1) Similarly, the number of true negatives (TN) is defined as the number of voxels assigned to the background in both s and t: TN = N X n=1 (1 - s n )(1 - t n ). (6.2) In contrast to these counts of voxels where both segmentations agree with one another, the number of false positives (FP) and the number of false negatives (FN) refer to segmentation errors made by the automated algorithm compared to the ground truth manual segmentation: in the former case, voxels are erro- neously assigned to the foreground when they really belong to the background, and in the latter case the exact opposite occurs: FP = N X n=1 s n (1 - t n ) (6.3) and FN = N X n=1 (1 - s n )t n . (6.4) The concepts of true positives, true negatives, false positives and false negatives are illustrated in Fig. 6.1. The four quantities defined in (6.1)– (6.4) can be summarized in a 2 × 2 table called the confusion matrix, defined as t 0 1 s 0 TN FN 1 FP TP . For segmentations that correspond perfectly with the ground truth, the off- diagonal elements FN and FP in the confusion matrix will be zero. We can also define the following relative quantities, ranging between 0 and 1: - The true positive rate TP TP+FN expresses the fraction of foreground voxels that were correctly identified as such. ","6.1. VALIDATION AGAINST A KNOWN GROUND TRUTH 71 Figure 6.1: Illustration of the concepts true and false positives and negatives. The “ground truth” segmentation t is shown in blue, and the to-be-evaluated segmentation s in red. - The true negative rate TN TN+FP expresses the fraction of background voxels that were correctly identified as such. - The false positive rate FP TN+FP expresses the fraction of background voxels that were incorrectly identified as foreground. - The false negative rate FN TP+FN expresses the fraction of foreground voxels that were incorrectly identified as background. The true positive rate is also commonly referred to as the sensitivity of a seg- mentation, and the true negative rate as the specificity. For segmentations that correspond perfectly with the ground truth, they will both have the maximum value of 1. 6.1.2 ROC curve In binary classifiers, aiming at correctly assigning binary labels to some input data, there is often a threshold value that determines the exact location of the classification boundary in feature space. In some image segmentation problems, for instance, a viable segmentation strategy might be to assign all voxels with an intensity above a certain threshold value to the foreground class, and the rest to the background. An illustration of this process is shown in Fig. 6.2, where some white matter affected by small vessel disease appears very bright in the MRI scan, and can therefore be segmented (to some extent) by simple thresholding. As shown in Fig. 6.2, the exact threshold value that is used plays a decisive role in the quality of the resulting segmentations. If the threshold value is chosen too high, only very bright voxels are selected, resulting in a very low false ","72 CHAPTER 6. VALIDATION (a) (b) (c) (d) Figure 6.2: In MR images acquired with the so-called FLAIR protocol, lesions in the white matter appear bright and can be segmented, to some extent, by simple intensity thresholding. Shown are a skull-stripped FLAIR scan (a); a manual segmentation that functions as the ground truth t (b); a segmentation s obtained by thresholding with a high threshold value (c); and with a low threshold value (d). ","6.1. VALIDATION AGAINST A KNOWN GROUND TRUTH 73 Figure 6.3: The ROC curve obtained by intensity thresholding the MR scan of Fig. 6.2a using a wide range of threshold values, and comparing the results to the manual segmentation shown in Fig. 6.2b. The red dot corresponds to the segmentation shown in Fig. 6.2c, the green dot to Fig. 6.2d, and the blue dot to the threshold setting that minimizes the Euclidean distance to the perfect classifier (point (0,1)). positive rate (which is good because it means very few voxels in the background are erroneously selected), but also in a low true positive rate (which is bad because it means many foreground voxels have been missed). On the other hand, choosing the threshold value too low results in an excellent true positive rate but at the expensive of an elevated false positive rate. For each setting of the threshold value, we can obtain a pair (false positive rate, true positive rate ) by comparing the resulting segmentation to a manual segmentation that is considered to be the ground truth (shown in Fig. 6.2b). The plot of these pairs over a whole range of threshold values is called the receiver operating characteristic (ROC), or simply ROC curve. The ROC curve corresponding to the thresholding experiment of Fig. 6.2 is shown in Fig. 6.3. Note that the point (0,1) corresponds to the (unattainable) perfect classifier: it classifies all foreground and background voxels correctly. In practice, the best threshold setting is always a compromise between the false positive rate and the true positive rate performance. If we do not know anything about the cost of misclassification (of either type) or the prior distribution of the classes, one strategy is to choose the threshold value that minimizes the Euclidean distance between (0,1) (the perfect classifier) and the corresponding location on the ROC curve. ","74 CHAPTER 6. VALIDATION 6.1.3 Dice score An often used segmentation performance metric that summarizes the overall spatial overlap between a segmentation s and a ground truth segmentation t is the so-called Dice score. It is defined as simply the volume of those voxels that are deemed foreground in both segmentations simultaneously, divided by the mean volume of the foreground in both segmentations. Since the volume of the foreground in s is given by TP + FP, and by TP + FN for t, the Dice score can be computed as Dice = TP (TP+FP)+(TP+FN) 2 = 2TP 2TP + FP + FN . It always lies between 0 (no spatial overlap between s and t at all) and 1 (perfect correspondence), and penalizes both false negatives and false positives at the same time. It is therefore very easy to report and interpret, making it a popular overall segmentation performance metric in the medical image segmentation literature. Several examples of alternate manual segmentations of the MR data of Fig. 6.2a are shown in Fig. 6.4, along with their Dice overlap scores with the first manual segmentation (Fig. 6.4a), which was considered the ground truth t for this purpose. Note that lower spatial correspondence (e.g., Fig. 6.4d) indeed results in a lower Dice overlap score. 6.2 Estimating the ground truth So far, we have assumed that a single manual segmentation performed by a human expert is the perfect ground truth segmentation, i.e., corresponds per- fectly to the underlying biology. In practice, however, there is often considerable disagreement between even highly trained experts on what the perfect segmen- tation of a given medical image should be. We already saw an example of this in Fig. 6.4, where different human raters segmented the same MRI scan and differed quite a bit in their judgment, in this case because the diffuseness of the lesions makes deciding on exact boundary locations particularly challenging. In scenarios like this, it is not clear which ground truth segmentations we should compare the results of new automated algorithms to. We know that some human experts might produce more accurate segmentations than others: some might “over-segment” (i.e., have high sensitivity but low specificity), others might “under-segment” (high specificity but low sensitivity), while yet others might just be sloppy (both low sensitivity and specificity). It turns out we can estimate the underlying ground truth t from M im- perfect manual segmentations by explicitly modeling the errors human experts are likely to make [12]. Let s m =(s 1,m ,...,s N,m ) T denote the mth available segmentation, where s n,m ∈{0, 1} indicates whether or not voxel n is assigned ","6.2. ESTIMATING THE GROUND TRUTH 75 (a) Dice score: 1 (b) Dice score: 0.78 (c) Dice score: 0.77 (d) Dice score: 0.66 Figure 6.4: Four alternate manual segmentations of the MR scan of Fig. 6.2a. The Dice scores are computed with respect to the first manual segmentation (a). ","76 CHAPTER 6. VALIDATION to the foreground by rater m. Assuming that rater m has sensitivity p m and specificity q m , and that (s)he makes labeling errors in each voxel independently, we have p(s m |t,p m ,q m )= N Y n=1 p(s n,m |t n ,p m ,q m ) (6.5) for the probability of s m , where p(s|t, p, q)=  p s (1 - p) (1-s) if t =1 q (1-s) (1 - q) s if t =0 . (6.6) (6.6) simply re-expresses the definition of sensitivity and specificity : if the ground truth label is 1, there is a probability p that the segmentation label will also be 1. Similarly, if the ground truth label is 0, the segmentation label will also be 0 with probability q. Let S =(s 1 ,..., s M ) denote all M available segmentations, and similarly θ =(p 1 ,...,p M ,q 1 ,...,q M ) T all the parameters of the model, consisting of the sensitivity and specificity of all raters, respectively. Since we can assume that each rater makes his/her segmentation independently of the other raters, we have that p(S|t, θ) = M Y m=1 p(s m |t,p m ,q m ) = M Y m=1 N Y n=1 p(s n,m |t n ,p m ,q m ) = N Y n=1 p(s n |t n , θ), (6.7) where we have defined p(s n |t n , θ)= M Y m=1 p(s N,m |t n ,p m ,q m ) with s n =(s n,1 ,...s n,M ) T (6.8) for mathematical convenience later on. In order to complete the model, we also specify a prior p(t) that expresses our prior expectations about the ground truth segmentation t. Similar to the prior used in the Gaussian mixture model (cf. (3.5)), we will use a simple prior of the form p(t)= Q N n=1 π tn , where π 1 and π 0 = (1 - π 1 ) govern the frequency with which voxels are expected to belong to foreground and background, respectively. In the remainder, we will assume that reasonable estimates of these parameters are known in advance, and keep their values fixed throughout. As was the case with the model-based segmentation models discussed in Chapter 3, suitable values for the model parameters θ can be obtained through maximum likelihood estimation. Also here we will perform the optimization ˆ θ = arg max θ log p(S|θ) ","6.2. ESTIMATING THE GROUND TRUTH 77 by using the EM algorithm, i.e., by repeatedly constructing a lower bound Q(θ| ˜ θ) that touches the log likelihood function at the current parameter esti- mate ˜ θ, and maximizing that lower bound, until convergence. Since the likeli- hood function is given by p(S|θ) = X t p(S|t, θ)p(t) = X t \" N Y n=1 p(s n |t n , θ) N Y n=1 π tn # = N Y n=1  p(s n |t n =0, θ)π 0 + p(s n |t n =1, θ)π 1  , we have that log p(S|θ) = N X n=1 log  p(s n |t n =0, θ)π 0 + p(s n |t n =1, θ)π 1  = N X n=1 log  p(s n |t n =0, θ)π 0 ω n,0  ω n,0 +  p(s n |t n =1, θ)π 1 ω n,1  ω n,1  ≥ N X n=1  ω n,0 log  p(s n |t n =0, θ)π 0 ω n,0  + ω n,1 log  p(s n |t n =1, θ)π 1 ω n,1  | {z } Q(θ| ˜ θ) , for any pair of weights {ω n,0 ,ω n,1 } in each voxel that satisfy ω n,0 + ω n,1 = 1 and ω n,0 ,ω n,1 ≥ 0 (the last step uses (3.28)). In order for Q(θ| ˜ θ) to additionally touch the log likelihood function at ˜ θ, these weights have to be chosen so that ω n,0 ∝ p(s n |t n =0, ˜ θ)π 0 and ω n,1 ∝ p(s n |t n =1, ˜ θ)π 1 , (6.9) as can be easily verified by substituting these weights into the definition of the lower bound and observing that then Q( ˜ θ| ˜ θ) = log p(S| ˜ θ). The EM algorithm now dictates that the parameter estimate ˜ θ be updated to the parameter vector that maximizes the lower bound. Re-writing the lower bound as Q(θ| ˜ θ) = M X m=1 \" N X n=1 ω n,0 (1 - s n,m ) ! log q m +  N X n=1 ω n,0 s n,m ! log(1 - q m ) # + M X m=1 \" N X n=1 ω n,1 s n,m ! log p m +  N X n=1 ω n,1 (1 - s n,m ) ! log(1 - p m ) # + N X n=1  ω n,0 log  π 0 ω n,0  + ω n,1 log  π 1 ω n,1  (6.10) ","78 CHAPTER 6. VALIDATION and requiring that ∂Q(θ| ˜ θ) ∂ θ =0 yields ˜ p m ← ∑ N n=1 ω n,1 s n,m ∑ N n=1 ω n,1 (6.11) ˜ q m ← ∑ N n=1 ω n,0 (1 - s n,m ) ∑ N n=1 ω n,0 (6.12) for the updates of the model parameters. In summary, the EM parameter optimizer iteratively computes the proba- bility with which each voxel belongs to the foreground or background based on the available segmentations and the current estimates of each rater’s sensitivity and specificity ((6.9)), and then updates each rater’s sensitivity and specificity accordingly ((6.11) and (6.12)). Interesting, if some rater is estimated to have low sensitivity and specificity, that rater’s segmentation is automatically down- weighted in the estimation of the foreground/background assignment probabili- ties, making the algorithm effectively focus on the most trustable segmentations only. Upon convergence of the EM algorithm, an estimate of the ground truth corresponding to the estimated parameters ˆ θ can be found by looking for the maximum a posteriori ground truth ˆ t = arg max t p(t|S, ˆ θ), which is obtained by assigning each voxel n to the foreground if ω n,1 ≥ 0.5 and to the background otherwise. This estimated ground truth can then be used as an unbiased reference segmentation to compare automated segmentation results to. Fig. 6.5 shows the underlying ground truth ˆ t estimated from the four manual segmentations shown in Fig. 6.4. The raters’ sensitivity ˆ p m and specificity ˆ q m were estimated by the algorithm to be 0.867 and 0.997 for the segmentation of Fig. 6.4a, 0.782 and 0.998 for Fig. 6.4b , 0.935 and 0.988 for Fig. 6.4c , and 0.512 and 0.999 for Fig. 6.4d. The absolute values of all the specificity estimates are high because of the sheer size of the background; it is their relative magnitudes that really matter. ","6.2. ESTIMATING THE GROUND TRUTH 79 Figure 6.5: Estimated ground truth ˆ t from the four manual segmentations of Fig. 6.4. The parameter π 1 was set to the average fraction of foreground voxels across all four manual segmentations, and π 0 = (1 - π 1 ). ","80 CHAPTER 6. VALIDATION ","Bibliography [1] M. Unser, “Splines: A perfect fit for signal and image processing,” IEEE Signal Processing Magazine, vol. 16, no. 6, pp. 22–38, 1999. [2] P. H. Sch¨onemann, “A generalized solution of the orthogonal procrustes problem,” Psychometrika, vol. 31, no. 1, pp. 1–10, 1966. [3] P. Th´evenaz, T. Blu, and M. Unser, “Interpolation revisited [medical im- ages application],” IEEE Transactions on medical imaging, vol. 19, no. 7, pp. 739–758, 2000. [4] W. M. Wells III, P. Viola, H. Atsumi, S. Nakajima, and R. Kikinis, “Multi- modal volume registration by maximization of mutual information,” Med- ical image analysis, vol. 1, no. 1, pp. 35–51, 1996. [5] F. Maes, A. Collignon, D. Vandermeulen, G. Marchal, and P. Suetens, “Multimodality image registration by maximization of mutual informa- tion,” IEEE transactions on Medical Imaging, vol. 16, no. 2, pp. 187–198, 1997. [6] D. Greig, B. Porteous, and A. Seheult, “Exact maximum a posteriori esti- mation for binary images,” Journal of the Royal Statistical Society. Series B (Methodological), vol. 51, no. 2, pp. 271–279, 1989. [7] T. Jaakkola, Advanced Mean Field Methods: Theory and Practice, ch. Tu- torial on Variational Approximation Methods. The MIT Press, 2001. [8] D. Hunter and K. Lange, “A tutorial on MM algorithms,” The American Statistician, vol. 58, no. 1, pp. 30–37, 2004. [9] C. M. Bishop, Pattern Recognition and Machine Learning. springer, 2006. [10] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, “Learning representa- tions by back-propagating errors,” Nature, vol. 323, no. 6088, pp. 533–536, 1986. [11] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hub- bard, and L. D. Jackel, “Backpropagation applied to handwritten zip code recognition,” Neural Computation, vol. 1, no. 4, pp. 541–551, 1989. 81 ","82 BIBLIOGRAPHY [12] S. Warfield, K. Zou, and W. Wells, “Simultaneous truth and performance level estimation (STAPLE): An algorithm for the validation of image seg- mentation,” IEEE Transactions on Medical Imaging, vol. 23, no. 7, pp. 903– 921, 2004. "]